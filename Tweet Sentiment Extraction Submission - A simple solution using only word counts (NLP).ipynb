{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### This is a simple solution using only word counts with CountVectorizer to make predictions.\n",
    "\n",
    "#### Here's the idea:\n",
    "- Find and weight words that are used most often in only certain kinds of tweets.\n",
    "- Search all subsets of the tweet and calculate a score based on these weights.\n",
    "- For positive or negative tweets, the selected text is the most highly weighted subset, within some threshold.\n",
    "- Always return the entire text for neutral tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "# CountVectorizer will help calculate word counts\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Import the string dictionary that we'll use to remove punctuation\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import datasets\n",
    "\n",
    "train = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')\n",
    "sample = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>fdb77c3752</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         textID text selected_text sentiment\n",
       "314  fdb77c3752  NaN           NaN   neutral"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The row with index 13133 has NaN text, so remove it from the dataset\n",
    "\n",
    "train[train['text'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(314, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Create a training set and a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Break up the training data into datasets where the sentiment is positive, neutral, or negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make all the text lowercase - casing doesn't matter when \n",
    "# we choose our selected text.\n",
    "train['text'] = train['text'].apply(lambda x: x.lower())\n",
    "test['text'] = test['text'].apply(lambda x: x.lower())\n",
    "\n",
    "# Make training/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val = train_test_split(\n",
    "    train, train_size = 0.80, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_train = X_train[X_train['sentiment'] == 'positive']\n",
    "neutral_train = X_train[X_train['sentiment'] == 'neutral']\n",
    "neg_train = X_train[X_train['sentiment'] == 'negative']"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### Algorithm for weight calculation:\n",
    "\n",
    "1. For each class $j \\in \\{positive, neutral, negative\\}$\n",
    "\n",
    "    a. Find all the words $i$ in the tweets belonging to class $j$.\n",
    "\n",
    "    b. Calculate $n_{i, j} =$ the number of tweets in class $j$ containing word $i$. \n",
    "\n",
    "    c. Let $d_j$ be the number of tweets in class $j$.  Calculate $p_{i, j} = \\frac{n_{i, j}}{d_j}$, the proportion of tweets in class $j$ that conain word $i$.\n",
    "\n",
    "    d. Let $w_{i, j} = p_{i, j} - \\sum\\limits_{k \\neq j}p_{i, k}$ be the weights assigned to each word within each class. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use CountVectorizer to get the word counts within each dataset\n",
    "\n",
    "cv = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                     max_features=10000,\n",
    "                                     stop_words='english')\n",
    "\n",
    "X_train_cv = cv.fit_transform(X_train['text'])\n",
    "\n",
    "X_pos = cv.transform(pos_train['text'])\n",
    "X_neutral = cv.transform(neutral_train['text'])\n",
    "X_neg = cv.transform(neg_train['text'])\n",
    "\n",
    "pos_count_df = pd.DataFrame(X_pos.toarray(), columns=cv.get_feature_names())\n",
    "neutral_count_df = pd.DataFrame(X_neutral.toarray(), columns=cv.get_feature_names())\n",
    "neg_count_df = pd.DataFrame(X_neg.toarray(), columns=cv.get_feature_names())\n",
    "\n",
    "# Create dictionaries of the words within each sentiment group, where the values are the proportions of tweets that \n",
    "# contain those words\n",
    "\n",
    "pos_words = {}\n",
    "neutral_words = {}\n",
    "neg_words = {}\n",
    "\n",
    "for k in cv.get_feature_names():\n",
    "    pos = pos_count_df[k].sum()\n",
    "    neutral = neutral_count_df[k].sum()\n",
    "    neg = neg_count_df[k].sum()\n",
    "    \n",
    "    pos_words[k] = pos/pos_train.shape[0]\n",
    "    neutral_words[k] = neutral/neutral_train.shape[0]\n",
    "    neg_words[k] = neg/neg_train.shape[0]\n",
    "    \n",
    "# We need to account for the fact that there will be a lot of words used in tweets of every sentiment.  \n",
    "# Therefore, we reassign the values in the dictionary by subtracting the proportion of tweets in the other \n",
    "# sentiments that use that word.\n",
    "\n",
    "neg_words_adj = {}\n",
    "pos_words_adj = {}\n",
    "neutral_words_adj = {}\n",
    "\n",
    "for key, value in neg_words.items():\n",
    "    neg_words_adj[key] = neg_words[key] - (neutral_words[key] + pos_words[key])\n",
    "    \n",
    "for key, value in pos_words.items():\n",
    "    pos_words_adj[key] = pos_words[key] - (neutral_words[key] + neg_words[key])\n",
    "    \n",
    "for key, value in neutral_words.items():\n",
    "    neutral_words_adj[key] = neutral_words[key] - (neg_words[key] + pos_words[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### Algorithm for finding selected text: \n",
    "  \n",
    "1. For every tweet:\n",
    "\n",
    "    a. Let $j$ be the sentiment of the tweet. \n",
    "\n",
    "    b. If $j ==$ neutral return entire text.\n",
    "\n",
    "    c. Otherwise, for each subset of words in the tweet, calculate $\\sum\\limits_{i}w_{i, j}$, where $i$ is the set of words in the tweet\n",
    "\n",
    "   d. Return the subset of words with the largest sum, given that it exceeds some tolerance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_selected_text(df_row, tol = 0):\n",
    "    \n",
    "    tweet = df_row['text']\n",
    "    sentiment = df_row['sentiment']\n",
    "    \n",
    "    if(sentiment == 'neutral'):\n",
    "        return tweet\n",
    "    \n",
    "    elif(sentiment == 'positive'):\n",
    "        dict_to_use = pos_words_adj # Calculate word weights using the pos_words dictionary\n",
    "    elif(sentiment == 'negative'):\n",
    "        dict_to_use = neg_words_adj # Calculate word weights using the neg_words dictionary\n",
    "        \n",
    "    words = tweet.split()\n",
    "    words_len = len(words)\n",
    "    subsets = [words[i:j+1] for i in range(words_len) for j in range(i,words_len)]\n",
    "    \n",
    "    score = 0\n",
    "    selection_str = '' # This will be our choice\n",
    "    lst = sorted(subsets, key = len) # Sort candidates by length\n",
    "    \n",
    "    \n",
    "    for i in range(len(subsets)):\n",
    "        \n",
    "        new_sum = 0 # Sum for the current substring\n",
    "        \n",
    "        # Calculate the sum of weights for each word in the substring\n",
    "        for p in range(len(lst[i])):\n",
    "            if(lst[i][p].translate(str.maketrans('','',string.punctuation)) in dict_to_use.keys()):\n",
    "                new_sum += dict_to_use[lst[i][p].translate(str.maketrans('','',string.punctuation))]\n",
    "            \n",
    "        # If the sum is greater than the score, update our current selection\n",
    "        if(new_sum > score + tol):\n",
    "            score = new_sum\n",
    "            selection_str = lst[i]\n",
    "            #tol = tol*5 # Increase the tolerance a bit each time we choose a selection\n",
    "\n",
    "    # If we didn't find good substrings, return the whole text\n",
    "    if(len(selection_str) == 0):\n",
    "        selection_str = words\n",
    "        \n",
    "    return ' '.join(selection_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Calculate the selected text and score for the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol = 0.001\n",
    "\n",
    "X_val['predicted_selection'] = ''\n",
    "\n",
    "for index, row in X_val.iterrows():\n",
    "    \n",
    "    selected_text = calculate_selected_text(row, tol)\n",
    "    \n",
    "    X_val.loc[X_val['textID'] == row['textID'], ['predicted_selection']] = selected_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The jaccard score for the validation set is: 0.6574824857797201\n"
     ]
    }
   ],
   "source": [
    "X_val['jaccard'] = X_val.apply(lambda x: jaccard(x['selected_text'], x['predicted_selection']), axis = 1)\n",
    "\n",
    "print('The jaccard score for the validation set is:', np.mean(X_val['jaccard']))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### Generate Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Recalculate word weights using the entire training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tr = train[train['sentiment'] == 'positive']\n",
    "neutral_tr = train[train['sentiment'] == 'neutral']\n",
    "neg_tr = train[train['sentiment'] == 'negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                     max_features=10000,\n",
    "                                     stop_words='english')\n",
    "\n",
    "final_cv = cv.fit_transform(train['text'])\n",
    "\n",
    "X_pos = cv.transform(pos_tr['text'])\n",
    "X_neutral = cv.transform(neutral_tr['text'])\n",
    "X_neg = cv.transform(neg_tr['text'])\n",
    "\n",
    "pos_final_count_df = pd.DataFrame(X_pos.toarray(), columns=cv.get_feature_names())\n",
    "neutral_final_count_df = pd.DataFrame(X_neutral.toarray(), columns=cv.get_feature_names())\n",
    "neg_final_count_df = pd.DataFrame(X_neg.toarray(), columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_words = {}\n",
    "neutral_words = {}\n",
    "neg_words = {}\n",
    "\n",
    "for k in cv.get_feature_names():\n",
    "    pos = pos_final_count_df[k].sum()\n",
    "    neutral = neutral_final_count_df[k].sum()\n",
    "    neg = neg_final_count_df[k].sum()\n",
    "    \n",
    "    pos_words[k] = pos/(pos_tr.shape[0])\n",
    "    neutral_words[k] = neutral/(neutral_tr.shape[0])\n",
    "    neg_words[k] = neg/(neg_tr.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_words_adj = {}\n",
    "pos_words_adj = {}\n",
    "neutral_words_adj = {}\n",
    "\n",
    "for key, value in neg_words.items():\n",
    "    neg_words_adj[key] = neg_words[key] - (neutral_words[key] + pos_words[key])\n",
    "    \n",
    "for key, value in pos_words.items():\n",
    "    pos_words_adj[key] = pos_words[key] - (neutral_words[key] + neg_words[key])\n",
    "    \n",
    "for key, value in neutral_words.items():\n",
    "    neutral_words_adj[key] = neutral_words[key] - (neg_words[key] + pos_words[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Create and submit the submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol = 0.001\n",
    "\n",
    "for index, row in test.iterrows():\n",
    "    \n",
    "    selected_text = calculate_selected_text(row, tol)\n",
    "    \n",
    "    sample.loc[sample['textID'] == row['textID'], ['selected_text']] = selected_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.to_csv('submission.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
