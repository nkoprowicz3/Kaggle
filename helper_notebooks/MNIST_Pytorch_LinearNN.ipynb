{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "batch_size = 200\n",
    "epochs = 10\n",
    "log_interval = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])),\n",
    "    batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    # override the init method to construct the neural net\n",
    "    def __init__(self):\n",
    "        # start by calling super\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 200)\n",
    "        self.fc2 = nn.Linear(200, 200)\n",
    "        self.fc3 = nn.Linear(200, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Feed the input to the first layer, then apply a relu to it\n",
    "        x = F.relu(self.fc1(x)) \n",
    "        \n",
    "        # Feed the input to the second layer, then apply a relu to it\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        # Feed the input to the third layer\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        # Usually use a softmax activation function in multi-class\n",
    "        # learning problems where a set of features can be related to \n",
    "        # one of k classes\n",
    "        \n",
    "        # The softmax activation normalizes the output vector so that the\n",
    "        # the sum of the vector is 1, which we can interpret as being\n",
    "        # the probabilities that the set of features belongs to each class\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=200, bias=True)\n",
      "  (fc2): Linear(in_features=200, out_features=200, bias=True)\n",
      "  (fc3): Linear(in_features=200, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get a summary of the network\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A stochastic gradient descent optimizer\n",
    "\n",
    "# The learning rate controls the amount that the weights are updated during training\n",
    "optimizer = optim.SGD(net.parameters(), lr = learning_rate, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a loss function\n",
    "# The loss function maps the parameter values to a scalar that\n",
    "# indicates how well the paramets accomplish the task of the network\n",
    "\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:28: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.292826\n",
      "Train Epoch: 0 [2000/60000 (3%)]\tLoss: 2.170783\n",
      "Train Epoch: 0 [4000/60000 (7%)]\tLoss: 1.884325\n",
      "Train Epoch: 0 [6000/60000 (10%)]\tLoss: 1.296913\n",
      "Train Epoch: 0 [8000/60000 (13%)]\tLoss: 0.858189\n",
      "Train Epoch: 0 [10000/60000 (17%)]\tLoss: 0.506540\n",
      "Train Epoch: 0 [12000/60000 (20%)]\tLoss: 0.465575\n",
      "Train Epoch: 0 [14000/60000 (23%)]\tLoss: 0.403011\n",
      "Train Epoch: 0 [16000/60000 (27%)]\tLoss: 0.491207\n",
      "Train Epoch: 0 [18000/60000 (30%)]\tLoss: 0.349354\n",
      "Train Epoch: 0 [20000/60000 (33%)]\tLoss: 0.400889\n",
      "Train Epoch: 0 [22000/60000 (37%)]\tLoss: 0.371908\n",
      "Train Epoch: 0 [24000/60000 (40%)]\tLoss: 0.361245\n",
      "Train Epoch: 0 [26000/60000 (43%)]\tLoss: 0.350378\n",
      "Train Epoch: 0 [28000/60000 (47%)]\tLoss: 0.282604\n",
      "Train Epoch: 0 [30000/60000 (50%)]\tLoss: 0.277913\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.407545\n",
      "Train Epoch: 0 [34000/60000 (57%)]\tLoss: 0.478580\n",
      "Train Epoch: 0 [36000/60000 (60%)]\tLoss: 0.378193\n",
      "Train Epoch: 0 [38000/60000 (63%)]\tLoss: 0.269544\n",
      "Train Epoch: 0 [40000/60000 (67%)]\tLoss: 0.231358\n",
      "Train Epoch: 0 [42000/60000 (70%)]\tLoss: 0.382991\n",
      "Train Epoch: 0 [44000/60000 (73%)]\tLoss: 0.285747\n",
      "Train Epoch: 0 [46000/60000 (77%)]\tLoss: 0.302238\n",
      "Train Epoch: 0 [48000/60000 (80%)]\tLoss: 0.207997\n",
      "Train Epoch: 0 [50000/60000 (83%)]\tLoss: 0.230658\n",
      "Train Epoch: 0 [52000/60000 (87%)]\tLoss: 0.265488\n",
      "Train Epoch: 0 [54000/60000 (90%)]\tLoss: 0.215075\n",
      "Train Epoch: 0 [56000/60000 (93%)]\tLoss: 0.312187\n",
      "Train Epoch: 0 [58000/60000 (97%)]\tLoss: 0.248476\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.285935\n",
      "Train Epoch: 1 [2000/60000 (3%)]\tLoss: 0.183315\n",
      "Train Epoch: 1 [4000/60000 (7%)]\tLoss: 0.176595\n",
      "Train Epoch: 1 [6000/60000 (10%)]\tLoss: 0.228071\n",
      "Train Epoch: 1 [8000/60000 (13%)]\tLoss: 0.311726\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 0.141965\n",
      "Train Epoch: 1 [12000/60000 (20%)]\tLoss: 0.266808\n",
      "Train Epoch: 1 [14000/60000 (23%)]\tLoss: 0.204077\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.204399\n",
      "Train Epoch: 1 [18000/60000 (30%)]\tLoss: 0.117952\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 0.274077\n",
      "Train Epoch: 1 [22000/60000 (37%)]\tLoss: 0.200067\n",
      "Train Epoch: 1 [24000/60000 (40%)]\tLoss: 0.152171\n",
      "Train Epoch: 1 [26000/60000 (43%)]\tLoss: 0.183650\n",
      "Train Epoch: 1 [28000/60000 (47%)]\tLoss: 0.264840\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 0.188365\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.148889\n",
      "Train Epoch: 1 [34000/60000 (57%)]\tLoss: 0.193973\n",
      "Train Epoch: 1 [36000/60000 (60%)]\tLoss: 0.182798\n",
      "Train Epoch: 1 [38000/60000 (63%)]\tLoss: 0.120125\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.124070\n",
      "Train Epoch: 1 [42000/60000 (70%)]\tLoss: 0.170062\n",
      "Train Epoch: 1 [44000/60000 (73%)]\tLoss: 0.136556\n",
      "Train Epoch: 1 [46000/60000 (77%)]\tLoss: 0.211038\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.095130\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 0.151677\n",
      "Train Epoch: 1 [52000/60000 (87%)]\tLoss: 0.172727\n",
      "Train Epoch: 1 [54000/60000 (90%)]\tLoss: 0.135866\n",
      "Train Epoch: 1 [56000/60000 (93%)]\tLoss: 0.200407\n",
      "Train Epoch: 1 [58000/60000 (97%)]\tLoss: 0.147926\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.204073\n",
      "Train Epoch: 2 [2000/60000 (3%)]\tLoss: 0.111143\n",
      "Train Epoch: 2 [4000/60000 (7%)]\tLoss: 0.175196\n",
      "Train Epoch: 2 [6000/60000 (10%)]\tLoss: 0.100752\n",
      "Train Epoch: 2 [8000/60000 (13%)]\tLoss: 0.145133\n",
      "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 0.167723\n",
      "Train Epoch: 2 [12000/60000 (20%)]\tLoss: 0.122947\n",
      "Train Epoch: 2 [14000/60000 (23%)]\tLoss: 0.165082\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.168770\n",
      "Train Epoch: 2 [18000/60000 (30%)]\tLoss: 0.159553\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 0.150868\n",
      "Train Epoch: 2 [22000/60000 (37%)]\tLoss: 0.106254\n",
      "Train Epoch: 2 [24000/60000 (40%)]\tLoss: 0.159810\n",
      "Train Epoch: 2 [26000/60000 (43%)]\tLoss: 0.171775\n",
      "Train Epoch: 2 [28000/60000 (47%)]\tLoss: 0.132669\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 0.129489\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.095929\n",
      "Train Epoch: 2 [34000/60000 (57%)]\tLoss: 0.199656\n",
      "Train Epoch: 2 [36000/60000 (60%)]\tLoss: 0.098562\n",
      "Train Epoch: 2 [38000/60000 (63%)]\tLoss: 0.085947\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.081277\n",
      "Train Epoch: 2 [42000/60000 (70%)]\tLoss: 0.150815\n",
      "Train Epoch: 2 [44000/60000 (73%)]\tLoss: 0.158791\n",
      "Train Epoch: 2 [46000/60000 (77%)]\tLoss: 0.084217\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.105801\n",
      "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 0.113328\n",
      "Train Epoch: 2 [52000/60000 (87%)]\tLoss: 0.150064\n",
      "Train Epoch: 2 [54000/60000 (90%)]\tLoss: 0.152140\n",
      "Train Epoch: 2 [56000/60000 (93%)]\tLoss: 0.070169\n",
      "Train Epoch: 2 [58000/60000 (97%)]\tLoss: 0.114148\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.122130\n",
      "Train Epoch: 3 [2000/60000 (3%)]\tLoss: 0.071020\n",
      "Train Epoch: 3 [4000/60000 (7%)]\tLoss: 0.041837\n",
      "Train Epoch: 3 [6000/60000 (10%)]\tLoss: 0.048481\n",
      "Train Epoch: 3 [8000/60000 (13%)]\tLoss: 0.151165\n",
      "Train Epoch: 3 [10000/60000 (17%)]\tLoss: 0.115777\n",
      "Train Epoch: 3 [12000/60000 (20%)]\tLoss: 0.092458\n",
      "Train Epoch: 3 [14000/60000 (23%)]\tLoss: 0.111893\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.131166\n",
      "Train Epoch: 3 [18000/60000 (30%)]\tLoss: 0.099319\n",
      "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 0.092233\n",
      "Train Epoch: 3 [22000/60000 (37%)]\tLoss: 0.094343\n",
      "Train Epoch: 3 [24000/60000 (40%)]\tLoss: 0.082007\n",
      "Train Epoch: 3 [26000/60000 (43%)]\tLoss: 0.049748\n",
      "Train Epoch: 3 [28000/60000 (47%)]\tLoss: 0.146973\n",
      "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 0.068695\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.105537\n",
      "Train Epoch: 3 [34000/60000 (57%)]\tLoss: 0.166750\n",
      "Train Epoch: 3 [36000/60000 (60%)]\tLoss: 0.082855\n",
      "Train Epoch: 3 [38000/60000 (63%)]\tLoss: 0.125223\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 0.062658\n",
      "Train Epoch: 3 [42000/60000 (70%)]\tLoss: 0.139537\n",
      "Train Epoch: 3 [44000/60000 (73%)]\tLoss: 0.126530\n",
      "Train Epoch: 3 [46000/60000 (77%)]\tLoss: 0.108370\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.082661\n",
      "Train Epoch: 3 [50000/60000 (83%)]\tLoss: 0.084547\n",
      "Train Epoch: 3 [52000/60000 (87%)]\tLoss: 0.143755\n",
      "Train Epoch: 3 [54000/60000 (90%)]\tLoss: 0.080839\n",
      "Train Epoch: 3 [56000/60000 (93%)]\tLoss: 0.118810\n",
      "Train Epoch: 3 [58000/60000 (97%)]\tLoss: 0.076047\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.083117\n",
      "Train Epoch: 4 [2000/60000 (3%)]\tLoss: 0.143875\n",
      "Train Epoch: 4 [4000/60000 (7%)]\tLoss: 0.090023\n",
      "Train Epoch: 4 [6000/60000 (10%)]\tLoss: 0.106616\n",
      "Train Epoch: 4 [8000/60000 (13%)]\tLoss: 0.046377\n",
      "Train Epoch: 4 [10000/60000 (17%)]\tLoss: 0.053256\n",
      "Train Epoch: 4 [12000/60000 (20%)]\tLoss: 0.065665\n",
      "Train Epoch: 4 [14000/60000 (23%)]\tLoss: 0.062826\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.072500\n",
      "Train Epoch: 4 [18000/60000 (30%)]\tLoss: 0.065958\n",
      "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 0.074472\n",
      "Train Epoch: 4 [22000/60000 (37%)]\tLoss: 0.089144\n",
      "Train Epoch: 4 [24000/60000 (40%)]\tLoss: 0.080404\n",
      "Train Epoch: 4 [26000/60000 (43%)]\tLoss: 0.078233\n",
      "Train Epoch: 4 [28000/60000 (47%)]\tLoss: 0.067718\n",
      "Train Epoch: 4 [30000/60000 (50%)]\tLoss: 0.083368\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.047068\n",
      "Train Epoch: 4 [34000/60000 (57%)]\tLoss: 0.046091\n",
      "Train Epoch: 4 [36000/60000 (60%)]\tLoss: 0.084556\n",
      "Train Epoch: 4 [38000/60000 (63%)]\tLoss: 0.134007\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 0.054071\n",
      "Train Epoch: 4 [42000/60000 (70%)]\tLoss: 0.107449\n",
      "Train Epoch: 4 [44000/60000 (73%)]\tLoss: 0.066743\n",
      "Train Epoch: 4 [46000/60000 (77%)]\tLoss: 0.097664\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.090428\n",
      "Train Epoch: 4 [50000/60000 (83%)]\tLoss: 0.057655\n",
      "Train Epoch: 4 [52000/60000 (87%)]\tLoss: 0.063973\n",
      "Train Epoch: 4 [54000/60000 (90%)]\tLoss: 0.044902\n",
      "Train Epoch: 4 [56000/60000 (93%)]\tLoss: 0.150656\n",
      "Train Epoch: 4 [58000/60000 (97%)]\tLoss: 0.043007\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.115800\n",
      "Train Epoch: 5 [2000/60000 (3%)]\tLoss: 0.094954\n",
      "Train Epoch: 5 [4000/60000 (7%)]\tLoss: 0.045681\n",
      "Train Epoch: 5 [6000/60000 (10%)]\tLoss: 0.051201\n",
      "Train Epoch: 5 [8000/60000 (13%)]\tLoss: 0.054449\n",
      "Train Epoch: 5 [10000/60000 (17%)]\tLoss: 0.029778\n",
      "Train Epoch: 5 [12000/60000 (20%)]\tLoss: 0.104327\n",
      "Train Epoch: 5 [14000/60000 (23%)]\tLoss: 0.093839\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.053930\n",
      "Train Epoch: 5 [18000/60000 (30%)]\tLoss: 0.056331\n",
      "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 0.044637\n",
      "Train Epoch: 5 [22000/60000 (37%)]\tLoss: 0.096471\n",
      "Train Epoch: 5 [24000/60000 (40%)]\tLoss: 0.071500\n",
      "Train Epoch: 5 [26000/60000 (43%)]\tLoss: 0.086652\n",
      "Train Epoch: 5 [28000/60000 (47%)]\tLoss: 0.095428\n",
      "Train Epoch: 5 [30000/60000 (50%)]\tLoss: 0.077886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.140291\n",
      "Train Epoch: 5 [34000/60000 (57%)]\tLoss: 0.061203\n",
      "Train Epoch: 5 [36000/60000 (60%)]\tLoss: 0.051077\n",
      "Train Epoch: 5 [38000/60000 (63%)]\tLoss: 0.058814\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 0.048409\n",
      "Train Epoch: 5 [42000/60000 (70%)]\tLoss: 0.057342\n",
      "Train Epoch: 5 [44000/60000 (73%)]\tLoss: 0.040080\n",
      "Train Epoch: 5 [46000/60000 (77%)]\tLoss: 0.075764\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.075504\n",
      "Train Epoch: 5 [50000/60000 (83%)]\tLoss: 0.099168\n",
      "Train Epoch: 5 [52000/60000 (87%)]\tLoss: 0.037767\n",
      "Train Epoch: 5 [54000/60000 (90%)]\tLoss: 0.097994\n",
      "Train Epoch: 5 [56000/60000 (93%)]\tLoss: 0.077418\n",
      "Train Epoch: 5 [58000/60000 (97%)]\tLoss: 0.044937\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.049461\n",
      "Train Epoch: 6 [2000/60000 (3%)]\tLoss: 0.078221\n",
      "Train Epoch: 6 [4000/60000 (7%)]\tLoss: 0.071145\n",
      "Train Epoch: 6 [6000/60000 (10%)]\tLoss: 0.160765\n",
      "Train Epoch: 6 [8000/60000 (13%)]\tLoss: 0.086118\n",
      "Train Epoch: 6 [10000/60000 (17%)]\tLoss: 0.038111\n",
      "Train Epoch: 6 [12000/60000 (20%)]\tLoss: 0.067831\n",
      "Train Epoch: 6 [14000/60000 (23%)]\tLoss: 0.083045\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.044869\n",
      "Train Epoch: 6 [18000/60000 (30%)]\tLoss: 0.042588\n",
      "Train Epoch: 6 [20000/60000 (33%)]\tLoss: 0.073773\n",
      "Train Epoch: 6 [22000/60000 (37%)]\tLoss: 0.104398\n",
      "Train Epoch: 6 [24000/60000 (40%)]\tLoss: 0.074119\n",
      "Train Epoch: 6 [26000/60000 (43%)]\tLoss: 0.081725\n",
      "Train Epoch: 6 [28000/60000 (47%)]\tLoss: 0.093265\n",
      "Train Epoch: 6 [30000/60000 (50%)]\tLoss: 0.081834\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.056755\n",
      "Train Epoch: 6 [34000/60000 (57%)]\tLoss: 0.087721\n",
      "Train Epoch: 6 [36000/60000 (60%)]\tLoss: 0.041257\n",
      "Train Epoch: 6 [38000/60000 (63%)]\tLoss: 0.044512\n",
      "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 0.029355\n",
      "Train Epoch: 6 [42000/60000 (70%)]\tLoss: 0.025114\n",
      "Train Epoch: 6 [44000/60000 (73%)]\tLoss: 0.081687\n",
      "Train Epoch: 6 [46000/60000 (77%)]\tLoss: 0.029827\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.046671\n",
      "Train Epoch: 6 [50000/60000 (83%)]\tLoss: 0.027581\n",
      "Train Epoch: 6 [52000/60000 (87%)]\tLoss: 0.038346\n",
      "Train Epoch: 6 [54000/60000 (90%)]\tLoss: 0.129740\n",
      "Train Epoch: 6 [56000/60000 (93%)]\tLoss: 0.059572\n",
      "Train Epoch: 6 [58000/60000 (97%)]\tLoss: 0.068361\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.099957\n",
      "Train Epoch: 7 [2000/60000 (3%)]\tLoss: 0.027001\n",
      "Train Epoch: 7 [4000/60000 (7%)]\tLoss: 0.041171\n",
      "Train Epoch: 7 [6000/60000 (10%)]\tLoss: 0.047931\n",
      "Train Epoch: 7 [8000/60000 (13%)]\tLoss: 0.064242\n",
      "Train Epoch: 7 [10000/60000 (17%)]\tLoss: 0.052043\n",
      "Train Epoch: 7 [12000/60000 (20%)]\tLoss: 0.037096\n",
      "Train Epoch: 7 [14000/60000 (23%)]\tLoss: 0.021902\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.026508\n",
      "Train Epoch: 7 [18000/60000 (30%)]\tLoss: 0.050234\n",
      "Train Epoch: 7 [20000/60000 (33%)]\tLoss: 0.022440\n",
      "Train Epoch: 7 [22000/60000 (37%)]\tLoss: 0.033702\n",
      "Train Epoch: 7 [24000/60000 (40%)]\tLoss: 0.067260\n",
      "Train Epoch: 7 [26000/60000 (43%)]\tLoss: 0.037581\n",
      "Train Epoch: 7 [28000/60000 (47%)]\tLoss: 0.089092\n",
      "Train Epoch: 7 [30000/60000 (50%)]\tLoss: 0.044773\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.038155\n",
      "Train Epoch: 7 [34000/60000 (57%)]\tLoss: 0.037825\n",
      "Train Epoch: 7 [36000/60000 (60%)]\tLoss: 0.063839\n",
      "Train Epoch: 7 [38000/60000 (63%)]\tLoss: 0.044571\n",
      "Train Epoch: 7 [40000/60000 (67%)]\tLoss: 0.036700\n",
      "Train Epoch: 7 [42000/60000 (70%)]\tLoss: 0.018693\n",
      "Train Epoch: 7 [44000/60000 (73%)]\tLoss: 0.022329\n",
      "Train Epoch: 7 [46000/60000 (77%)]\tLoss: 0.104733\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.028941\n",
      "Train Epoch: 7 [50000/60000 (83%)]\tLoss: 0.090994\n",
      "Train Epoch: 7 [52000/60000 (87%)]\tLoss: 0.094179\n",
      "Train Epoch: 7 [54000/60000 (90%)]\tLoss: 0.090710\n",
      "Train Epoch: 7 [56000/60000 (93%)]\tLoss: 0.033096\n",
      "Train Epoch: 7 [58000/60000 (97%)]\tLoss: 0.026062\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.034077\n",
      "Train Epoch: 8 [2000/60000 (3%)]\tLoss: 0.051104\n",
      "Train Epoch: 8 [4000/60000 (7%)]\tLoss: 0.045725\n",
      "Train Epoch: 8 [6000/60000 (10%)]\tLoss: 0.069979\n",
      "Train Epoch: 8 [8000/60000 (13%)]\tLoss: 0.075088\n",
      "Train Epoch: 8 [10000/60000 (17%)]\tLoss: 0.067332\n",
      "Train Epoch: 8 [12000/60000 (20%)]\tLoss: 0.079120\n",
      "Train Epoch: 8 [14000/60000 (23%)]\tLoss: 0.049280\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.024528\n",
      "Train Epoch: 8 [18000/60000 (30%)]\tLoss: 0.025411\n",
      "Train Epoch: 8 [20000/60000 (33%)]\tLoss: 0.044479\n",
      "Train Epoch: 8 [22000/60000 (37%)]\tLoss: 0.026994\n",
      "Train Epoch: 8 [24000/60000 (40%)]\tLoss: 0.033333\n",
      "Train Epoch: 8 [26000/60000 (43%)]\tLoss: 0.079375\n",
      "Train Epoch: 8 [28000/60000 (47%)]\tLoss: 0.021556\n",
      "Train Epoch: 8 [30000/60000 (50%)]\tLoss: 0.048153\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.041424\n",
      "Train Epoch: 8 [34000/60000 (57%)]\tLoss: 0.028650\n",
      "Train Epoch: 8 [36000/60000 (60%)]\tLoss: 0.098600\n",
      "Train Epoch: 8 [38000/60000 (63%)]\tLoss: 0.057732\n",
      "Train Epoch: 8 [40000/60000 (67%)]\tLoss: 0.018857\n",
      "Train Epoch: 8 [42000/60000 (70%)]\tLoss: 0.051065\n",
      "Train Epoch: 8 [44000/60000 (73%)]\tLoss: 0.045126\n",
      "Train Epoch: 8 [46000/60000 (77%)]\tLoss: 0.029932\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.044915\n",
      "Train Epoch: 8 [50000/60000 (83%)]\tLoss: 0.045045\n",
      "Train Epoch: 8 [52000/60000 (87%)]\tLoss: 0.040268\n",
      "Train Epoch: 8 [54000/60000 (90%)]\tLoss: 0.022458\n",
      "Train Epoch: 8 [56000/60000 (93%)]\tLoss: 0.067040\n",
      "Train Epoch: 8 [58000/60000 (97%)]\tLoss: 0.044896\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.020392\n",
      "Train Epoch: 9 [2000/60000 (3%)]\tLoss: 0.015045\n",
      "Train Epoch: 9 [4000/60000 (7%)]\tLoss: 0.056753\n",
      "Train Epoch: 9 [6000/60000 (10%)]\tLoss: 0.057811\n",
      "Train Epoch: 9 [8000/60000 (13%)]\tLoss: 0.044001\n",
      "Train Epoch: 9 [10000/60000 (17%)]\tLoss: 0.066305\n",
      "Train Epoch: 9 [12000/60000 (20%)]\tLoss: 0.007439\n",
      "Train Epoch: 9 [14000/60000 (23%)]\tLoss: 0.019192\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.030414\n",
      "Train Epoch: 9 [18000/60000 (30%)]\tLoss: 0.031191\n",
      "Train Epoch: 9 [20000/60000 (33%)]\tLoss: 0.011715\n",
      "Train Epoch: 9 [22000/60000 (37%)]\tLoss: 0.019193\n",
      "Train Epoch: 9 [24000/60000 (40%)]\tLoss: 0.051856\n",
      "Train Epoch: 9 [26000/60000 (43%)]\tLoss: 0.037601\n",
      "Train Epoch: 9 [28000/60000 (47%)]\tLoss: 0.019380\n",
      "Train Epoch: 9 [30000/60000 (50%)]\tLoss: 0.042213\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.017061\n",
      "Train Epoch: 9 [34000/60000 (57%)]\tLoss: 0.028669\n",
      "Train Epoch: 9 [36000/60000 (60%)]\tLoss: 0.043529\n",
      "Train Epoch: 9 [38000/60000 (63%)]\tLoss: 0.082626\n",
      "Train Epoch: 9 [40000/60000 (67%)]\tLoss: 0.017323\n",
      "Train Epoch: 9 [42000/60000 (70%)]\tLoss: 0.050479\n",
      "Train Epoch: 9 [44000/60000 (73%)]\tLoss: 0.016335\n",
      "Train Epoch: 9 [46000/60000 (77%)]\tLoss: 0.033526\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.027890\n",
      "Train Epoch: 9 [50000/60000 (83%)]\tLoss: 0.093643\n",
      "Train Epoch: 9 [52000/60000 (87%)]\tLoss: 0.052345\n",
      "Train Epoch: 9 [54000/60000 (90%)]\tLoss: 0.018730\n",
      "Train Epoch: 9 [56000/60000 (93%)]\tLoss: 0.022642\n",
      "Train Epoch: 9 [58000/60000 (97%)]\tLoss: 0.015474\n"
     ]
    }
   ],
   "source": [
    "# Each epoch is a complete pass through the dataset\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        # Variable class wraps a tensor and allows automatic\n",
    "        # gradient computation on the tensor when the backward function\n",
    "        # is called.  It contains the data of the tensor, the gradient of\n",
    "        # the tensor, and a reference to whatever function created the variable\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        \n",
    "        # resize the data from (batch_size, 1, 28, 28) to (batch_size, 28*28)\n",
    "        data = data.view(-1, 28*28)\n",
    "        \n",
    "        # for each batch, need to zero out the gradient\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # apply the network\n",
    "        net_out = net(data)\n",
    "        loss = criterion(net_out, target) \n",
    "        loss.backward() # compute the gradient\n",
    "        optimizer.step() # step in the direction of the gradient (update the network parameters)\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                           100. * batch_idx / len(train_loader), loss.data))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one example from the test dataset\n",
    "\n",
    "data, target = [ x[0] for x in iter(test_loader).next() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target # this is the correct answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:28: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "data, target = Variable(data, volatile=True), Variable(target)\n",
    "data = data.view(-1, 28 * 28)\n",
    "net_out = net(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.1919e+01, -1.2988e+01, -1.6888e-03, -6.7287e+00, -1.4883e+01,\n",
       "         -1.6335e+01, -2.1311e+01, -7.6240e+00, -2.0260e+01, -2.6327e+01]],\n",
       "       grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_out.argmax() # This is what the neural network predicts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
