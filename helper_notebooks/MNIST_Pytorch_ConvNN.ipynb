{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import show\n",
    "from bokeh.models import LinearAxis, Range1d\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "batch_size = 100\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "\n",
    "DATA_PATH = r'\\Users\\Nick\\Desktop\\Learning ML\\Tutorials\\MNIST PyTorch\\MNISTData'\n",
    "MODEL_STORE_PATH = r'\\Users\\Nick\\Desktop\\Learning ML\\Tutorials\\MNIST PyTorch\\MNISTData\\Model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to \\Users\\Nick\\Desktop\\Learning ML\\Tutorials\\MNIST PyTorch\\MNISTData/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9920512it [00:02, 4668187.12it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting \\Users\\Nick\\Desktop\\Learning ML\\Tutorials\\MNIST PyTorch\\MNISTData/MNIST/raw/train-images-idx3-ubyte.gz to \\Users\\Nick\\Desktop\\Learning ML\\Tutorials\\MNIST PyTorch\\MNISTData/MNIST/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/28881 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to \\Users\\Nick\\Desktop\\Learning ML\\Tutorials\\MNIST PyTorch\\MNISTData/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32768it [00:00, 160057.10it/s]           \n",
      "  0%|          | 0/1648877 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting \\Users\\Nick\\Desktop\\Learning ML\\Tutorials\\MNIST PyTorch\\MNISTData/MNIST/raw/train-labels-idx1-ubyte.gz to \\Users\\Nick\\Desktop\\Learning ML\\Tutorials\\MNIST PyTorch\\MNISTData/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to \\Users\\Nick\\Desktop\\Learning ML\\Tutorials\\MNIST PyTorch\\MNISTData/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1654784it [00:01, 1344351.66it/s]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting \\Users\\Nick\\Desktop\\Learning ML\\Tutorials\\MNIST PyTorch\\MNISTData/MNIST/raw/t10k-images-idx3-ubyte.gz to \\Users\\Nick\\Desktop\\Learning ML\\Tutorials\\MNIST PyTorch\\MNISTData/MNIST/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8192it [00:00, 55498.69it/s]            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to \\Users\\Nick\\Desktop\\Learning ML\\Tutorials\\MNIST PyTorch\\MNISTData/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Extracting \\Users\\Nick\\Desktop\\Learning ML\\Tutorials\\MNIST PyTorch\\MNISTData/MNIST/raw/t10k-labels-idx1-ubyte.gz to \\Users\\Nick\\Desktop\\Learning ML\\Tutorials\\MNIST PyTorch\\MNISTData/MNIST/raw\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# transforms to apply to the data\n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "# MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root=DATA_PATH, train=True, transform=trans, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root=DATA_PATH, train=False, transform=trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: \\Users\\Nick\\Desktop\\Learning ML\\Tutorials\\MNIST PyTorch\\MNISTData\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.1307,), std=(0.3081,))\n",
       "           )"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle = True)\n",
    "test_loader = DataLoader(dataset = test_dataset, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        # The input dimensions must be implied - we only specify the incoming\n",
    "        # and outgoing number of channels in each layer\n",
    "        \n",
    "        # Each layer has a convolution, a ReLU activation, and a pooling layer\n",
    "        self.layer1 = nn.Sequential(\n",
    "            \n",
    "            # 1 is the number of channels in the input, and 32 is the number of channels in the output\n",
    "            # ** Question: how are the 32 filters created, specifically? Are they\n",
    "            # pre-programmed in torch?\n",
    "            nn.Conv2d(1, 32, kernel_size = 5, stride = 1, padding = 2),\n",
    "            \n",
    "            # ReLU keeps learned parameters from getting stuck near 0, \n",
    "            # ob lowing up to infinity\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # pooling reduces the number of parameters we need to learn\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size = 5, stride = 1, padding = 2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        \n",
    "        # add a dropout layer to prevent over-fitting\n",
    "        self.drop_out = nn.Dropout()\n",
    "        \n",
    "        # Fully-connected layers\n",
    "        self.fc1 = nn.Linear(7*7*64, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "\n",
    "        # flatten the output\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "\n",
    "        out = self.drop_out(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When to use one loss vs another?\n",
    "# When to use one optimizer vs another?\n",
    "\n",
    "# Use cross-entropy loss for classification problems\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/600], Loss: 0.1586, Accuracy: 95.00%\n",
      "Epoch [1/5], Step [200/600], Loss: 0.2013, Accuracy: 94.00%\n",
      "Epoch [1/5], Step [300/600], Loss: 0.2614, Accuracy: 93.00%\n",
      "Epoch [1/5], Step [400/600], Loss: 0.0533, Accuracy: 99.00%\n",
      "Epoch [1/5], Step [500/600], Loss: 0.1639, Accuracy: 93.00%\n",
      "Epoch [1/5], Step [600/600], Loss: 0.2160, Accuracy: 95.00%\n",
      "Epoch [2/5], Step [100/600], Loss: 0.1163, Accuracy: 95.00%\n",
      "Epoch [2/5], Step [200/600], Loss: 0.0944, Accuracy: 97.00%\n",
      "Epoch [2/5], Step [300/600], Loss: 0.0424, Accuracy: 98.00%\n",
      "Epoch [2/5], Step [400/600], Loss: 0.1600, Accuracy: 98.00%\n",
      "Epoch [2/5], Step [500/600], Loss: 0.1934, Accuracy: 95.00%\n",
      "Epoch [2/5], Step [600/600], Loss: 0.0837, Accuracy: 96.00%\n",
      "Epoch [3/5], Step [100/600], Loss: 0.0747, Accuracy: 96.00%\n",
      "Epoch [3/5], Step [200/600], Loss: 0.0896, Accuracy: 97.00%\n",
      "Epoch [3/5], Step [300/600], Loss: 0.0598, Accuracy: 98.00%\n",
      "Epoch [3/5], Step [400/600], Loss: 0.0707, Accuracy: 97.00%\n",
      "Epoch [3/5], Step [500/600], Loss: 0.0640, Accuracy: 97.00%\n",
      "Epoch [3/5], Step [600/600], Loss: 0.0760, Accuracy: 99.00%\n",
      "Epoch [4/5], Step [100/600], Loss: 0.0598, Accuracy: 98.00%\n",
      "Epoch [4/5], Step [200/600], Loss: 0.0677, Accuracy: 98.00%\n",
      "Epoch [4/5], Step [300/600], Loss: 0.0503, Accuracy: 99.00%\n",
      "Epoch [4/5], Step [400/600], Loss: 0.1106, Accuracy: 96.00%\n",
      "Epoch [4/5], Step [500/600], Loss: 0.0488, Accuracy: 99.00%\n",
      "Epoch [4/5], Step [600/600], Loss: 0.0530, Accuracy: 99.00%\n",
      "Epoch [5/5], Step [100/600], Loss: 0.0243, Accuracy: 100.00%\n",
      "Epoch [5/5], Step [200/600], Loss: 0.0086, Accuracy: 100.00%\n",
      "Epoch [5/5], Step [300/600], Loss: 0.0009, Accuracy: 100.00%\n",
      "Epoch [5/5], Step [400/600], Loss: 0.0609, Accuracy: 96.00%\n",
      "Epoch [5/5], Step [500/600], Loss: 0.0588, Accuracy: 97.00%\n",
      "Epoch [5/5], Step [600/600], Loss: 0.0405, Accuracy: 98.00%\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_list.append(loss.item())\n",
    "        \n",
    "        # do backprop and step forward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track the accuracy\n",
    "        total = labels.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        acc_list.append(correct / total)\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
    "                  .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),\n",
    "                          (correct / total) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.3085732460021973,\n",
       " 4.266198635101318,\n",
       " 2.3248226642608643,\n",
       " 2.1178548336029053,\n",
       " 2.1375582218170166,\n",
       " 2.0911998748779297,\n",
       " 1.9743396043777466,\n",
       " 1.866123914718628,\n",
       " 1.8232954740524292,\n",
       " 1.698809027671814,\n",
       " 1.50838303565979,\n",
       " 1.3309123516082764,\n",
       " 1.2584373950958252,\n",
       " 1.0189917087554932,\n",
       " 1.0921680927276611,\n",
       " 0.8304089903831482,\n",
       " 0.8815823197364807,\n",
       " 0.8123769164085388,\n",
       " 0.6732941269874573,\n",
       " 0.678486168384552,\n",
       " 0.8088133931159973,\n",
       " 0.5870787501335144,\n",
       " 0.8281004428863525,\n",
       " 0.5033937692642212,\n",
       " 0.5520946383476257,\n",
       " 0.9426037073135376,\n",
       " 0.48066917061805725,\n",
       " 0.5877964496612549,\n",
       " 0.5544633269309998,\n",
       " 0.5281856656074524,\n",
       " 0.4316374957561493,\n",
       " 0.365474671125412,\n",
       " 0.3871639668941498,\n",
       " 0.3633579909801483,\n",
       " 0.3447851240634918,\n",
       " 0.5049936175346375,\n",
       " 0.3858660161495209,\n",
       " 0.2695232629776001,\n",
       " 0.39952027797698975,\n",
       " 0.24616193771362305,\n",
       " 0.33683040738105774,\n",
       " 0.27180150151252747,\n",
       " 0.3865392804145813,\n",
       " 0.4790905714035034,\n",
       " 0.28266769647598267,\n",
       " 0.16707399487495422,\n",
       " 0.29590731859207153,\n",
       " 0.3247138559818268,\n",
       " 0.25035908818244934,\n",
       " 0.35581886768341064,\n",
       " 0.321358859539032,\n",
       " 0.22425979375839233,\n",
       " 0.23229742050170898,\n",
       " 0.2593272924423218,\n",
       " 0.5015954375267029,\n",
       " 0.45274871587753296,\n",
       " 0.3127468526363373,\n",
       " 0.3421330153942108,\n",
       " 0.3341853618621826,\n",
       " 0.41327953338623047,\n",
       " 0.19795027375221252,\n",
       " 0.29498034715652466,\n",
       " 0.24233441054821014,\n",
       " 0.3046792149543762,\n",
       " 0.23619875311851501,\n",
       " 0.34354016184806824,\n",
       " 0.1469314992427826,\n",
       " 0.221610888838768,\n",
       " 0.248550146818161,\n",
       " 0.228940948843956,\n",
       " 0.16523174941539764,\n",
       " 0.2770351767539978,\n",
       " 0.16579608619213104,\n",
       " 0.27189871668815613,\n",
       " 0.19020578265190125,\n",
       " 0.2584949731826782,\n",
       " 0.1906450241804123,\n",
       " 0.10624227672815323,\n",
       " 0.2022067904472351,\n",
       " 0.21879273653030396,\n",
       " 0.26608192920684814,\n",
       " 0.28664207458496094,\n",
       " 0.30864596366882324,\n",
       " 0.09772688150405884,\n",
       " 0.11016812920570374,\n",
       " 0.21510006487369537,\n",
       " 0.23487091064453125,\n",
       " 0.17188358306884766,\n",
       " 0.20346476137638092,\n",
       " 0.20649108290672302,\n",
       " 0.3426370620727539,\n",
       " 0.3361496329307556,\n",
       " 0.2403479516506195,\n",
       " 0.2278895229101181,\n",
       " 0.2278464287519455,\n",
       " 0.1545657217502594,\n",
       " 0.17631065845489502,\n",
       " 0.25255703926086426,\n",
       " 0.10314716398715973,\n",
       " 0.158628448843956,\n",
       " 0.17206601798534393,\n",
       " 0.15410825610160828,\n",
       " 0.22325056791305542,\n",
       " 0.17104780673980713,\n",
       " 0.12068067491054535,\n",
       " 0.14817160367965698,\n",
       " 0.1719001829624176,\n",
       " 0.2905394732952118,\n",
       " 0.15695300698280334,\n",
       " 0.1568346917629242,\n",
       " 0.2821893095970154,\n",
       " 0.21223165094852448,\n",
       " 0.1774892359972,\n",
       " 0.27326443791389465,\n",
       " 0.145429328083992,\n",
       " 0.10286983847618103,\n",
       " 0.19650712609291077,\n",
       " 0.2390105426311493,\n",
       " 0.2508522868156433,\n",
       " 0.12138350307941437,\n",
       " 0.2482130229473114,\n",
       " 0.12466879934072495,\n",
       " 0.1746891736984253,\n",
       " 0.2691650390625,\n",
       " 0.1305297613143921,\n",
       " 0.243125781416893,\n",
       " 0.13559849560260773,\n",
       " 0.06884998083114624,\n",
       " 0.12835580110549927,\n",
       " 0.11638140678405762,\n",
       " 0.08215891569852829,\n",
       " 0.18479163944721222,\n",
       " 0.13703975081443787,\n",
       " 0.07157270610332489,\n",
       " 0.2213161289691925,\n",
       " 0.2852509617805481,\n",
       " 0.11256501078605652,\n",
       " 0.10633285343647003,\n",
       " 0.15826573967933655,\n",
       " 0.1883912980556488,\n",
       " 0.11297392845153809,\n",
       " 0.35422688722610474,\n",
       " 0.2337624877691269,\n",
       " 0.1769106537103653,\n",
       " 0.13585993647575378,\n",
       " 0.1964603066444397,\n",
       " 0.08502708375453949,\n",
       " 0.11123153567314148,\n",
       " 0.31928759813308716,\n",
       " 0.23467287421226501,\n",
       " 0.19274693727493286,\n",
       " 0.2051827758550644,\n",
       " 0.20785214006900787,\n",
       " 0.10474453866481781,\n",
       " 0.08826147019863129,\n",
       " 0.06674617528915405,\n",
       " 0.16502684354782104,\n",
       " 0.07843693345785141,\n",
       " 0.1254407912492752,\n",
       " 0.13582132756710052,\n",
       " 0.21016085147857666,\n",
       " 0.1941012144088745,\n",
       " 0.07844384759664536,\n",
       " 0.20053516328334808,\n",
       " 0.19248820841312408,\n",
       " 0.03966379538178444,\n",
       " 0.07331244647502899,\n",
       " 0.05342665687203407,\n",
       " 0.17853309214115143,\n",
       " 0.2225506603717804,\n",
       " 0.2580420672893524,\n",
       " 0.12598161399364471,\n",
       " 0.0653063952922821,\n",
       " 0.15700282156467438,\n",
       " 0.07649463415145874,\n",
       " 0.12892989814281464,\n",
       " 0.14740923047065735,\n",
       " 0.1327837109565735,\n",
       " 0.276386559009552,\n",
       " 0.11624325066804886,\n",
       " 0.16796499490737915,\n",
       " 0.20587807893753052,\n",
       " 0.1439615786075592,\n",
       " 0.277396559715271,\n",
       " 0.15315060317516327,\n",
       " 0.13733118772506714,\n",
       " 0.18897157907485962,\n",
       " 0.08522084355354309,\n",
       " 0.14511576294898987,\n",
       " 0.10764017701148987,\n",
       " 0.128461554646492,\n",
       " 0.23274579644203186,\n",
       " 0.13735832273960114,\n",
       " 0.10597158223390579,\n",
       " 0.20371057093143463,\n",
       " 0.17840488255023956,\n",
       " 0.24329692125320435,\n",
       " 0.11355111747980118,\n",
       " 0.19234822690486908,\n",
       " 0.20128518342971802,\n",
       " 0.2828877568244934,\n",
       " 0.17308861017227173,\n",
       " 0.22004938125610352,\n",
       " 0.08806528896093369,\n",
       " 0.10137020796537399,\n",
       " 0.13220711052417755,\n",
       " 0.30664485692977905,\n",
       " 0.20729009807109833,\n",
       " 0.4017743170261383,\n",
       " 0.2551283538341522,\n",
       " 0.19133472442626953,\n",
       " 0.15991593897342682,\n",
       " 0.1872544139623642,\n",
       " 0.1637992262840271,\n",
       " 0.15622110664844513,\n",
       " 0.35361263155937195,\n",
       " 0.12564624845981598,\n",
       " 0.09257201850414276,\n",
       " 0.08874455094337463,\n",
       " 0.1761370599269867,\n",
       " 0.1657748818397522,\n",
       " 0.2262515276670456,\n",
       " 0.1736043393611908,\n",
       " 0.19056417047977448,\n",
       " 0.20500868558883667,\n",
       " 0.12881289422512054,\n",
       " 0.149285688996315,\n",
       " 0.12938947975635529,\n",
       " 0.07092177867889404,\n",
       " 0.2257424294948578,\n",
       " 0.18426679074764252,\n",
       " 0.0902334526181221,\n",
       " 0.11270016431808472,\n",
       " 0.10536830127239227,\n",
       " 0.10302113741636276,\n",
       " 0.08753746747970581,\n",
       " 0.13047532737255096,\n",
       " 0.18372812867164612,\n",
       " 0.16405323147773743,\n",
       " 0.24042260646820068,\n",
       " 0.07694461196660995,\n",
       " 0.1303708404302597,\n",
       " 0.11080285906791687,\n",
       " 0.05119093507528305,\n",
       " 0.12196606397628784,\n",
       " 0.10844414681196213,\n",
       " 0.05151410400867462,\n",
       " 0.055163826793432236,\n",
       " 0.1543017029762268,\n",
       " 0.07314803451299667,\n",
       " 0.22805573046207428,\n",
       " 0.1123998761177063,\n",
       " 0.08663605898618698,\n",
       " 0.19604088366031647,\n",
       " 0.15643613040447235,\n",
       " 0.15437723696231842,\n",
       " 0.26088792085647583,\n",
       " 0.1468820422887802,\n",
       " 0.1672513335943222,\n",
       " 0.048331066966056824,\n",
       " 0.10767650604248047,\n",
       " 0.0977654829621315,\n",
       " 0.10835132747888565,\n",
       " 0.11552510410547256,\n",
       " 0.22131910920143127,\n",
       " 0.09286726266145706,\n",
       " 0.08119110763072968,\n",
       " 0.1513468623161316,\n",
       " 0.05208003893494606,\n",
       " 0.08164362609386444,\n",
       " 0.08058690279722214,\n",
       " 0.14345782995224,\n",
       " 0.13925524055957794,\n",
       " 0.10412526875734329,\n",
       " 0.05729667842388153,\n",
       " 0.1269247829914093,\n",
       " 0.12068308889865875,\n",
       " 0.1447094827890396,\n",
       " 0.10467567294836044,\n",
       " 0.18814903497695923,\n",
       " 0.05756642669439316,\n",
       " 0.11752936989068985,\n",
       " 0.059484612196683884,\n",
       " 0.14584821462631226,\n",
       " 0.17088626325130463,\n",
       " 0.0477236844599247,\n",
       " 0.14147861301898956,\n",
       " 0.04888327419757843,\n",
       " 0.07146258652210236,\n",
       " 0.14716795086860657,\n",
       " 0.06670452654361725,\n",
       " 0.16138416528701782,\n",
       " 0.18219493329524994,\n",
       " 0.059702567756175995,\n",
       " 0.17421479523181915,\n",
       " 0.12968280911445618,\n",
       " 0.09901168197393417,\n",
       " 0.10187219828367233,\n",
       " 0.1727464646100998,\n",
       " 0.2613983452320099,\n",
       " 0.12317746877670288,\n",
       " 0.08971426635980606,\n",
       " 0.17470796406269073,\n",
       " 0.1530078500509262,\n",
       " 0.05475281551480293,\n",
       " 0.08572029322385788,\n",
       " 0.05290963500738144,\n",
       " 0.1188308447599411,\n",
       " 0.21912112832069397,\n",
       " 0.08094225823879242,\n",
       " 0.14824466407299042,\n",
       " 0.05796585604548454,\n",
       " 0.065980464220047,\n",
       " 0.13845564424991608,\n",
       " 0.09015506505966187,\n",
       " 0.14452172815799713,\n",
       " 0.15004540979862213,\n",
       " 0.06441719084978104,\n",
       " 0.0732131227850914,\n",
       " 0.11171954870223999,\n",
       " 0.0997568890452385,\n",
       " 0.04839612916111946,\n",
       " 0.0908091738820076,\n",
       " 0.13246504962444305,\n",
       " 0.06385304033756256,\n",
       " 0.06844661384820938,\n",
       " 0.06891585141420364,\n",
       " 0.09697134047746658,\n",
       " 0.12656167149543762,\n",
       " 0.1628122180700302,\n",
       " 0.10620945692062378,\n",
       " 0.13956256210803986,\n",
       " 0.12842990458011627,\n",
       " 0.3374636769294739,\n",
       " 0.10405083000659943,\n",
       " 0.08537901192903519,\n",
       " 0.09477347135543823,\n",
       " 0.0656202957034111,\n",
       " 0.07653640955686569,\n",
       " 0.10339473932981491,\n",
       " 0.11350981891155243,\n",
       " 0.12226930260658264,\n",
       " 0.21801835298538208,\n",
       " 0.06812164932489395,\n",
       " 0.026678571477532387,\n",
       " 0.07425514608621597,\n",
       " 0.07613140344619751,\n",
       " 0.07595982402563095,\n",
       " 0.08621947467327118,\n",
       " 0.013261489570140839,\n",
       " 0.11903825402259827,\n",
       " 0.06277042627334595,\n",
       " 0.13754430413246155,\n",
       " 0.06509202718734741,\n",
       " 0.11161884665489197,\n",
       " 0.07426347583532333,\n",
       " 0.09881258010864258,\n",
       " 0.16847309470176697,\n",
       " 0.09326516091823578,\n",
       " 0.09761906415224075,\n",
       " 0.17526426911354065,\n",
       " 0.1664130985736847,\n",
       " 0.07289522886276245,\n",
       " 0.1726110279560089,\n",
       " 0.25628387928009033,\n",
       " 0.1274796724319458,\n",
       " 0.04054141044616699,\n",
       " 0.1922450065612793,\n",
       " 0.17784133553504944,\n",
       " 0.23168215155601501,\n",
       " 0.06641422212123871,\n",
       " 0.060864828526973724,\n",
       " 0.12692180275917053,\n",
       " 0.2027776539325714,\n",
       " 0.1973060667514801,\n",
       " 0.13877251744270325,\n",
       " 0.12421786040067673,\n",
       " 0.13155502080917358,\n",
       " 0.10173963755369186,\n",
       " 0.14857450127601624,\n",
       " 0.21336379647254944,\n",
       " 0.11268281936645508,\n",
       " 0.2964460849761963,\n",
       " 0.18583831191062927,\n",
       " 0.09603400528430939,\n",
       " 0.1304691731929779,\n",
       " 0.03851698338985443,\n",
       " 0.22563663125038147,\n",
       " 0.22006265819072723,\n",
       " 0.06963326781988144,\n",
       " 0.05873170495033264,\n",
       " 0.17536988854408264,\n",
       " 0.10732055455446243,\n",
       " 0.1933237612247467,\n",
       " 0.20178665220737457,\n",
       " 0.1195390596985817,\n",
       " 0.11214236915111542,\n",
       " 0.1981523633003235,\n",
       " 0.14885744452476501,\n",
       " 0.05329567939043045,\n",
       " 0.08447009325027466,\n",
       " 0.1488531231880188,\n",
       " 0.0416501946747303,\n",
       " 0.11727543920278549,\n",
       " 0.09644103795289993,\n",
       " 0.12474778294563293,\n",
       " 0.16639217734336853,\n",
       " 0.08757323026657104,\n",
       " 0.14474964141845703,\n",
       " 0.05059404671192169,\n",
       " 0.06648940593004227,\n",
       " 0.11463312804698944,\n",
       " 0.045494407415390015,\n",
       " 0.036118488758802414,\n",
       " 0.061929162591695786,\n",
       " 0.1275516003370285,\n",
       " 0.08482164144515991,\n",
       " 0.08735396713018417,\n",
       " 0.08710809797048569,\n",
       " 0.07776203751564026,\n",
       " 0.0922422781586647,\n",
       " 0.060507602989673615,\n",
       " 0.044118572026491165,\n",
       " 0.15035668015480042,\n",
       " 0.09544821083545685,\n",
       " 0.2374764084815979,\n",
       " 0.10507693141698837,\n",
       " 0.14348961412906647,\n",
       " 0.029506156221032143,\n",
       " 0.19094067811965942,\n",
       " 0.1786538064479828,\n",
       " 0.09688442945480347,\n",
       " 0.12109556794166565,\n",
       " 0.16883698105812073,\n",
       " 0.18419688940048218,\n",
       " 0.15207067131996155,\n",
       " 0.07742041349411011,\n",
       " 0.12970112264156342,\n",
       " 0.12770205736160278,\n",
       " 0.06464433670043945,\n",
       " 0.18229226768016815,\n",
       " 0.117624431848526,\n",
       " 0.09123959392309189,\n",
       " 0.15922829508781433,\n",
       " 0.07826730608940125,\n",
       " 0.13731226325035095,\n",
       " 0.11094263941049576,\n",
       " 0.023514525964856148,\n",
       " 0.17465700209140778,\n",
       " 0.12064732611179352,\n",
       " 0.11812551319599152,\n",
       " 0.07257871329784393,\n",
       " 0.060074806213378906,\n",
       " 0.0871477797627449,\n",
       " 0.12979656457901,\n",
       " 0.07283203303813934,\n",
       " 0.06775925308465958,\n",
       " 0.18318919837474823,\n",
       " 0.028405511751770973,\n",
       " 0.13848817348480225,\n",
       " 0.07197379320859909,\n",
       " 0.07354173064231873,\n",
       " 0.10855178534984589,\n",
       " 0.0686439722776413,\n",
       " 0.10823216289281845,\n",
       " 0.06520099937915802,\n",
       " 0.03267030790448189,\n",
       " 0.21426112949848175,\n",
       " 0.12214586138725281,\n",
       " 0.18005406856536865,\n",
       " 0.08272600173950195,\n",
       " 0.13640469312667847,\n",
       " 0.024077706038951874,\n",
       " 0.14249031245708466,\n",
       " 0.06253001093864441,\n",
       " 0.16226068139076233,\n",
       " 0.08451234549283981,\n",
       " 0.1542607843875885,\n",
       " 0.07024224102497101,\n",
       " 0.04207298159599304,\n",
       " 0.08268570154905319,\n",
       " 0.19800788164138794,\n",
       " 0.2197171151638031,\n",
       " 0.17727115750312805,\n",
       " 0.25727972388267517,\n",
       " 0.08227754384279251,\n",
       " 0.1102321594953537,\n",
       " 0.10627347230911255,\n",
       " 0.18229497969150543,\n",
       " 0.051572684198617935,\n",
       " 0.2186843752861023,\n",
       " 0.1881888061761856,\n",
       " 0.17633682489395142,\n",
       " 0.059853099286556244,\n",
       " 0.06581532210111618,\n",
       " 0.07060844451189041,\n",
       " 0.06709178537130356,\n",
       " 0.14501194655895233,\n",
       " 0.21541473269462585,\n",
       " 0.16385771334171295,\n",
       " 0.0883362665772438,\n",
       " 0.07527653872966766,\n",
       " 0.11989082396030426,\n",
       " 0.07461731135845184,\n",
       " 0.083417147397995,\n",
       " 0.12901265919208527,\n",
       " 0.15223120152950287,\n",
       " 0.09499774128198624,\n",
       " 0.13842281699180603,\n",
       " 0.024952232837677002,\n",
       " 0.08857566118240356,\n",
       " 0.14340049028396606,\n",
       " 0.043040528893470764,\n",
       " 0.1618635356426239,\n",
       " 0.08247958868741989,\n",
       " 0.03318978101015091,\n",
       " 0.1303855925798416,\n",
       " 0.0175184179097414,\n",
       " 0.0999976247549057,\n",
       " 0.20607878267765045,\n",
       " 0.23188218474388123,\n",
       " 0.12080646306276321,\n",
       " 0.10733474791049957,\n",
       " 0.02299361675977707,\n",
       " 0.08991140127182007,\n",
       " 0.10287903994321823,\n",
       " 0.11074625700712204,\n",
       " 0.16354265809059143,\n",
       " 0.1338585764169693,\n",
       " 0.06538690626621246,\n",
       " 0.11593069136142731,\n",
       " 0.07063907384872437,\n",
       " 0.08958645910024643,\n",
       " 0.09844686836004257,\n",
       " 0.04065190628170967,\n",
       " 0.2161373645067215,\n",
       " 0.13416843116283417,\n",
       " 0.10029210895299911,\n",
       " 0.06982860714197159,\n",
       " 0.10134676843881607,\n",
       " 0.05599994584918022,\n",
       " 0.24378859996795654,\n",
       " 0.0697864294052124,\n",
       " 0.07859113067388535,\n",
       " 0.10747867822647095,\n",
       " 0.13793744146823883,\n",
       " 0.04705517739057541,\n",
       " 0.12204501032829285,\n",
       " 0.10121556371450424,\n",
       " 0.10423068702220917,\n",
       " 0.2264678031206131,\n",
       " 0.15199366211891174,\n",
       " 0.10898641496896744,\n",
       " 0.0618320070207119,\n",
       " 0.06169271841645241,\n",
       " 0.13715338706970215,\n",
       " 0.09671848267316818,\n",
       " 0.03470992669463158,\n",
       " 0.0818297490477562,\n",
       " 0.1045290157198906,\n",
       " 0.03792645409703255,\n",
       " 0.06408689171075821,\n",
       " 0.07155533134937286,\n",
       " 0.08425126224756241,\n",
       " 0.053220558911561966,\n",
       " 0.015473517589271069,\n",
       " 0.15159843862056732,\n",
       " 0.055910009890794754,\n",
       " 0.1921481192111969,\n",
       " 0.09179676324129105,\n",
       " 0.10521847754716873,\n",
       " 0.10500086098909378,\n",
       " 0.09043005853891373,\n",
       " 0.11619679629802704,\n",
       " 0.025597987696528435,\n",
       " 0.05589904636144638,\n",
       " 0.07277389615774155,\n",
       " 0.04408465325832367,\n",
       " 0.1787695735692978,\n",
       " 0.06111342832446098,\n",
       " 0.04689430817961693,\n",
       " 0.1590506136417389,\n",
       " 0.07760121673345566,\n",
       " 0.07551097124814987,\n",
       " 0.12424633651971817,\n",
       " 0.019697828218340874,\n",
       " 0.042949870228767395,\n",
       " 0.3015768527984619,\n",
       " 0.20621797442436218,\n",
       " 0.18182680010795593,\n",
       " 0.08195576071739197,\n",
       " 0.09666729718446732,\n",
       " 0.1591084748506546,\n",
       " 0.12428294867277145,\n",
       " 0.08564997464418411,\n",
       " 0.15517711639404297,\n",
       " 0.11199096590280533,\n",
       " 0.06103973090648651,\n",
       " 0.08147719502449036,\n",
       " 0.2160474956035614,\n",
       " 0.11289733648300171,\n",
       " 0.08671103417873383,\n",
       " 0.0401151217520237,\n",
       " 0.0390528067946434,\n",
       " 0.07020754367113113,\n",
       " 0.157046839594841,\n",
       " 0.038778480142354965,\n",
       " 0.08807022869586945,\n",
       " 0.10591907799243927,\n",
       " 0.10537446290254593,\n",
       " 0.03525625169277191,\n",
       " 0.2318633645772934,\n",
       " 0.1807529777288437,\n",
       " 0.04912959039211273,\n",
       " 0.09053560346364975,\n",
       " 0.042170971632003784,\n",
       " 0.11715616285800934,\n",
       " 0.1465664505958557,\n",
       " 0.08995162695646286,\n",
       " 0.08300549536943436,\n",
       " 0.14568643271923065,\n",
       " 0.1953611522912979,\n",
       " 0.02352011203765869,\n",
       " 0.0542914904654026,\n",
       " 0.045976556837558746,\n",
       " 0.08719954639673233,\n",
       " 0.1404515504837036,\n",
       " 0.11403561383485794,\n",
       " 0.11336671561002731,\n",
       " 0.0935480147600174,\n",
       " 0.1950778365135193,\n",
       " 0.04502143710851669,\n",
       " 0.10030445456504822,\n",
       " 0.11398638784885406,\n",
       " 0.10203152894973755,\n",
       " 0.20539234578609467,\n",
       " 0.09437635540962219,\n",
       " 0.049822043627500534,\n",
       " 0.16309520602226257,\n",
       " 0.11209668219089508,\n",
       " 0.061142392456531525,\n",
       " 0.052197325974702835,\n",
       " 0.11663120985031128,\n",
       " 0.09922195225954056,\n",
       " 0.08891851454973221,\n",
       " 0.14472492039203644,\n",
       " 0.02136641927063465,\n",
       " 0.03346535563468933,\n",
       " 0.07170219719409943,\n",
       " 0.10749836266040802,\n",
       " 0.08153822273015976,\n",
       " 0.039132729172706604,\n",
       " 0.05808981880545616,\n",
       " 0.03796093165874481,\n",
       " 0.12115155160427094,\n",
       " 0.15350791811943054,\n",
       " 0.07338517159223557,\n",
       " 0.06276573985815048,\n",
       " 0.04115716367959976,\n",
       " 0.18319641053676605,\n",
       " 0.08982154726982117,\n",
       " 0.13849757611751556,\n",
       " 0.0969962403178215,\n",
       " 0.1241847425699234,\n",
       " 0.03243028372526169,\n",
       " 0.17367631196975708,\n",
       " 0.071419358253479,\n",
       " 0.019522113725543022,\n",
       " 0.05409376695752144,\n",
       " 0.12709695100784302,\n",
       " 0.10912010073661804,\n",
       " 0.03878730908036232,\n",
       " 0.05032731965184212,\n",
       " 0.06353884935379028,\n",
       " 0.05067470669746399,\n",
       " 0.11432895809412003,\n",
       " 0.12383382022380829,\n",
       " 0.08587857335805893,\n",
       " 0.10610881447792053,\n",
       " 0.13594427704811096,\n",
       " 0.04227534681558609,\n",
       " 0.13165117800235748,\n",
       " 0.12118222564458847,\n",
       " 0.06545672565698624,\n",
       " 0.12323865294456482,\n",
       " 0.08091234415769577,\n",
       " 0.057884249836206436,\n",
       " 0.15302123129367828,\n",
       " 0.07244762033224106,\n",
       " 0.19527515769004822,\n",
       " 0.11878803372383118,\n",
       " 0.10472096502780914,\n",
       " 0.11149127781391144,\n",
       " 0.08982669562101364,\n",
       " 0.07740353792905807,\n",
       " 0.04079340398311615,\n",
       " 0.12680216133594513,\n",
       " 0.11123611778020859,\n",
       " 0.07242831587791443,\n",
       " 0.11634345352649689,\n",
       " 0.14914682507514954,\n",
       " 0.1596909761428833,\n",
       " 0.08732682466506958,\n",
       " 0.16684721410274506,\n",
       " 0.0878215804696083,\n",
       " 0.09761511534452438,\n",
       " 0.04923442006111145,\n",
       " 0.07827717065811157,\n",
       " 0.13876353204250336,\n",
       " 0.11215734481811523,\n",
       " 0.10434693098068237,\n",
       " 0.13323992490768433,\n",
       " 0.06665261089801788,\n",
       " 0.1583360731601715,\n",
       " 0.08764179050922394,\n",
       " 0.07363981008529663,\n",
       " 0.08966274559497833,\n",
       " 0.07373624294996262,\n",
       " 0.04321783035993576,\n",
       " 0.05428770184516907,\n",
       " 0.022785356268286705,\n",
       " 0.06646597385406494,\n",
       " 0.14306986331939697,\n",
       " 0.07185233384370804,\n",
       " 0.07446049898862839,\n",
       " 0.2651623487472534,\n",
       " 0.018798377364873886,\n",
       " 0.14792893826961517,\n",
       " 0.07898341119289398,\n",
       " 0.08608172088861465,\n",
       " 0.0935300812125206,\n",
       " 0.04460950940847397,\n",
       " 0.13557621836662292,\n",
       " 0.09166264533996582,\n",
       " 0.039418939501047134,\n",
       " 0.07558075338602066,\n",
       " 0.07819225639104843,\n",
       " 0.1263740360736847,\n",
       " 0.24995344877243042,\n",
       " 0.09707364439964294,\n",
       " 0.047915827482938766,\n",
       " 0.16035886108875275,\n",
       " 0.09615502506494522,\n",
       " 0.06986252963542938,\n",
       " 0.07775776833295822,\n",
       " 0.09053514152765274,\n",
       " 0.1265619695186615,\n",
       " 0.15049876272678375,\n",
       " 0.1507505476474762,\n",
       " 0.1539403796195984,\n",
       " 0.03128407895565033,\n",
       " 0.09255950152873993,\n",
       " 0.14230825006961823,\n",
       " 0.054404884576797485,\n",
       " 0.17902417480945587,\n",
       " 0.1651090383529663,\n",
       " 0.06321123242378235,\n",
       " 0.03483926132321358,\n",
       " 0.03189815208315849,\n",
       " 0.08173708617687225,\n",
       " 0.13831552863121033,\n",
       " 0.049725037068128586,\n",
       " 0.07566240429878235,\n",
       " 0.09913769364356995,\n",
       " 0.1289384812116623,\n",
       " 0.02907034568488598,\n",
       " 0.08324424922466278,\n",
       " 0.12223294377326965,\n",
       " 0.12116070836782455,\n",
       " 0.07146596163511276,\n",
       " 0.06232732906937599,\n",
       " 0.04544667899608612,\n",
       " 0.06814561039209366,\n",
       " 0.059398338198661804,\n",
       " 0.07548131793737411,\n",
       " 0.03184746205806732,\n",
       " 0.054923828691244125,\n",
       " 0.2006397396326065,\n",
       " 0.09703569114208221,\n",
       " 0.09304986894130707,\n",
       " 0.2176591157913208,\n",
       " 0.07714001834392548,\n",
       " 0.046469565480947495,\n",
       " 0.08593709766864777,\n",
       " 0.015039211139082909,\n",
       " 0.13517411053180695,\n",
       " 0.1276281774044037,\n",
       " 0.10222885012626648,\n",
       " 0.13430973887443542,\n",
       " 0.048367418348789215,\n",
       " 0.08135580271482468,\n",
       " 0.1235521137714386,\n",
       " 0.10535291582345963,\n",
       " 0.11422099173069,\n",
       " 0.07993943989276886,\n",
       " 0.15858188271522522,\n",
       " 0.08064631372690201,\n",
       " 0.21652427315711975,\n",
       " 0.12456043064594269,\n",
       " 0.09438765794038773,\n",
       " 0.05593126639723778,\n",
       " 0.09637157618999481,\n",
       " 0.07835085690021515,\n",
       " 0.044920358806848526,\n",
       " 0.08086790144443512,\n",
       " 0.1440010517835617,\n",
       " 0.09306374192237854,\n",
       " 0.08656369149684906,\n",
       " 0.045875005424022675,\n",
       " 0.11314965039491653,\n",
       " 0.06775923818349838,\n",
       " 0.1820341944694519,\n",
       " 0.11372549086809158,\n",
       " 0.01960756629705429,\n",
       " 0.058935049921274185,\n",
       " 0.17444631457328796,\n",
       " 0.02483298070728779,\n",
       " 0.21434448659420013,\n",
       " 0.030915195122361183,\n",
       " 0.1322682499885559,\n",
       " 0.13219527900218964,\n",
       " 0.07226841151714325,\n",
       " 0.04391111060976982,\n",
       " 0.06627951562404633,\n",
       " 0.07892704755067825,\n",
       " 0.0652497261762619,\n",
       " 0.09592275321483612,\n",
       " 0.13362903892993927,\n",
       " 0.06841466575860977,\n",
       " 0.16459746658802032,\n",
       " 0.049457669258117676,\n",
       " 0.080561563372612,\n",
       " 0.1428225189447403,\n",
       " 0.021142851561307907,\n",
       " 0.1116083562374115,\n",
       " 0.03710883483290672,\n",
       " 0.035937584936618805,\n",
       " 0.06857936084270477,\n",
       " 0.10370271652936935,\n",
       " 0.13128894567489624,\n",
       " 0.037697967141866684,\n",
       " 0.06165391951799393,\n",
       " 0.04735809192061424,\n",
       " 0.055358950048685074,\n",
       " 0.07181553542613983,\n",
       " 0.057091206312179565,\n",
       " 0.0720292404294014,\n",
       " 0.051104702055454254,\n",
       " 0.07178600877523422,\n",
       " 0.07245085388422012,\n",
       " 0.04444863647222519,\n",
       " 0.046508584171533585,\n",
       " 0.10237915068864822,\n",
       " 0.06541092693805695,\n",
       " 0.03367209434509277,\n",
       " 0.021576248109340668,\n",
       " 0.14362891018390656,\n",
       " 0.07805013656616211,\n",
       " 0.02191472239792347,\n",
       " 0.08556580543518066,\n",
       " 0.05925650894641876,\n",
       " 0.0738748237490654,\n",
       " 0.023257846012711525,\n",
       " 0.06554868817329407,\n",
       " 0.0701865702867508,\n",
       " 0.046026717871427536,\n",
       " 0.18544882535934448,\n",
       " 0.18126578629016876,\n",
       " 0.06419506669044495,\n",
       " 0.08936300128698349,\n",
       " 0.03360935300588608,\n",
       " 0.11617449671030045,\n",
       " 0.0523231066763401,\n",
       " 0.15042844414710999,\n",
       " 0.10691239684820175,\n",
       " 0.03553759306669235,\n",
       " 0.07999470084905624,\n",
       " 0.04739178717136383,\n",
       " 0.07699405401945114,\n",
       " 0.05984912067651749,\n",
       " 0.10143548995256424,\n",
       " 0.09259578585624695,\n",
       " 0.14887464046478271,\n",
       " 0.035433050245046616,\n",
       " 0.0719568133354187,\n",
       " 0.0772511437535286,\n",
       " 0.12960398197174072,\n",
       " 0.055853791534900665,\n",
       " 0.1598871350288391,\n",
       " 0.026182109490036964,\n",
       " 0.09668650478124619,\n",
       " 0.1463509500026703,\n",
       " 0.09810692071914673,\n",
       " 0.0783679261803627,\n",
       " 0.05842107906937599,\n",
       " 0.022546742111444473,\n",
       " 0.0472707562148571,\n",
       " 0.11195261776447296,\n",
       " 0.06579426676034927,\n",
       " 0.04242151230573654,\n",
       " 0.1292145848274231,\n",
       " 0.0893164575099945,\n",
       " 0.10107672959566116,\n",
       " 0.0752268061041832,\n",
       " 0.02897069975733757,\n",
       " 0.1845695674419403,\n",
       " 0.1096784695982933,\n",
       " 0.038481879979372025,\n",
       " 0.17458218336105347,\n",
       " 0.13700728118419647,\n",
       " 0.02846456691622734,\n",
       " 0.12811873853206635,\n",
       " 0.01749253086745739,\n",
       " 0.14781542122364044,\n",
       " 0.07444170117378235,\n",
       " 0.12383341044187546,\n",
       " 0.06740602850914001,\n",
       " 0.14006486535072327,\n",
       " 0.12707622349262238,\n",
       " 0.15540792047977448,\n",
       " 0.04011030122637749,\n",
       " 0.051264457404613495,\n",
       " 0.05260748788714409,\n",
       " 0.08487740159034729,\n",
       " 0.09347414970397949,\n",
       " 0.051547933369874954,\n",
       " 0.16771696507930756,\n",
       " 0.04226358234882355,\n",
       " 0.061933454126119614,\n",
       " 0.05465991795063019,\n",
       " 0.04715694114565849,\n",
       " 0.08068257570266724,\n",
       " 0.07604990154504776,\n",
       " 0.06773271411657333,\n",
       " 0.058762431144714355,\n",
       " 0.11245512217283249,\n",
       " 0.03431297838687897,\n",
       " 0.09687153995037079,\n",
       " 0.08351278305053711,\n",
       " 0.027250045910477638,\n",
       " 0.06585197150707245,\n",
       " 0.057179976254701614,\n",
       " 0.06747201085090637,\n",
       " 0.28659525513648987,\n",
       " 0.06571545451879501,\n",
       " 0.0541195347905159,\n",
       " 0.06395640224218369,\n",
       " 0.06633083522319794,\n",
       " 0.11575507372617722,\n",
       " 0.25111106038093567,\n",
       " 0.0748932808637619,\n",
       " 0.13420948386192322,\n",
       " 0.0428658202290535,\n",
       " 0.03640831634402275,\n",
       " 0.07009580731391907,\n",
       " 0.298737496137619,\n",
       " 0.1541988104581833,\n",
       " 0.09396710991859436,\n",
       " 0.07308967411518097,\n",
       " 0.01711924374103546,\n",
       " 0.10467440634965897,\n",
       " 0.11509277671575546,\n",
       " 0.0380154587328434,\n",
       " 0.08260257542133331,\n",
       " 0.07517766207456589,\n",
       " 0.07013680785894394,\n",
       " 0.13759168982505798,\n",
       " 0.06573735922574997,\n",
       " 0.06930786371231079,\n",
       " 0.07360811531543732,\n",
       " 0.1712484359741211,\n",
       " 0.07883463054895401,\n",
       " 0.027431264519691467,\n",
       " 0.029693538323044777,\n",
       " 0.037728168070316315,\n",
       " 0.08746736496686935,\n",
       " 0.06366460025310516,\n",
       " 0.15784664452075958,\n",
       " 0.11959529668092728,\n",
       " 0.03719822317361832,\n",
       " 0.0400463342666626,\n",
       " 0.1583019345998764,\n",
       " 0.06947272270917892,\n",
       " 0.028063470497727394,\n",
       " 0.03678290918469429,\n",
       " 0.03810107335448265,\n",
       " 0.1620304435491562,\n",
       " 0.03864050284028053,\n",
       " 0.03191044181585312,\n",
       " 0.11087723076343536,\n",
       " 0.04678809642791748,\n",
       " 0.20848499238491058,\n",
       " 0.15044377744197845,\n",
       " 0.10952844470739365,\n",
       " 0.018853390589356422,\n",
       " 0.027736740186810493,\n",
       " 0.2020990252494812,\n",
       " 0.07388274371623993,\n",
       " 0.15025556087493896,\n",
       " 0.16001658141613007,\n",
       " ...]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 99.05000000000001 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format((correct / total) * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), MODEL_STORE_PATH + 'conv_net_model.ckpt')\n",
    "\n",
    "p = figure(y_axis_label='Loss', width=850, y_range=(0, 1), title='PyTorch ConvNet results')\n",
    "p.extra_y_ranges = {'Accuracy': Range1d(start=0, end=100)}\n",
    "p.add_layout(LinearAxis(y_range_name='Accuracy', axis_label='Accuracy (%)'), 'right')\n",
    "p.line(np.arange(len(loss_list)), loss_list)\n",
    "p.line(np.arange(len(loss_list)), np.array(acc_list) * 100, y_range_name='Accuracy', color='red')\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one example from the test dataset\n",
    "data, target = [ x[0] for x in iter(test_loader).next() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = data.view(-1, 1, 28, 28)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data, target = Variable(data, volatile=True), Variable(target)\n",
    "#data = data.view(-1, 28 * 28)\n",
    "net_out = model(data)\n",
    "net_out.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([23.8073]),\n",
       "indices=tensor([7]))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(net_out.data, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
