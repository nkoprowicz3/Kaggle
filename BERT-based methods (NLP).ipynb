{"cells":[{"metadata":{},"cell_type":"markdown","source":"# BERT-based models: Step by step"},{"metadata":{},"cell_type":"markdown","source":"## BERT-base models are **transformers**. They transform text into *contextualized embeddings*: vector representations of sentences that capture each words' semantic meaning."},{"metadata":{},"cell_type":"markdown","source":"![](https://imgur.com/dFCbxsY.jpg)"},{"metadata":{},"cell_type":"markdown","source":"## So we use these models to \"read\" text. Once the text is \"read\" we can do three tasks: classify based on sentiment, answer questions, or named-entity regocnition."},{"metadata":{},"cell_type":"markdown","source":"## Task 1: Classification"},{"metadata":{},"cell_type":"markdown","source":"![](https://imgur.com/pDMub6B.jpg)"},{"metadata":{},"cell_type":"markdown","source":"## Task 2: Question Answering"},{"metadata":{},"cell_type":"markdown","source":"![](https://imgur.com/MzIIHCp.jpg)"},{"metadata":{},"cell_type":"markdown","source":"## Task 3: Named Entity Recognition"},{"metadata":{},"cell_type":"markdown","source":"# Example 1: Classification"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# Step 1: Import data, split into training and validation set (nothing specific to BERT)"},{"metadata":{},"cell_type":"markdown","source":"Just reading in the data here, as we would for any other maching learning task"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\ntrain = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\ntest = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')\nsample = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/sample_submission.csv')\n\ntrain = train.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert text labels to integers\nfrom sklearn import preprocessing\n\nfeatures = train['text']\n\nencoder = preprocessing.LabelEncoder()\ny = encoder.fit_transform(train['sentiment'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split into a training set and a validation set\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(features, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 2: Create inputs that the BERT model can read"},{"metadata":{},"cell_type":"markdown","source":"![](https://imgur.com/PITly25.jpg)"},{"metadata":{},"cell_type":"markdown","source":"## 2a. Tokenization"},{"metadata":{},"cell_type":"markdown","source":"It's not necessarily the case that each word in a text gets its own tokenization. Though it is often true, the tokenizer needs to be able to handle text that it's never seen before. For example, consider misspellings, or a text like \"asdfa\". \n\nThe way that text is encoded is by use of a **vocab file** and a **merges file**. The way it works is: text is broken down to the character level, then it's merged into larger pieces (sometimes, but not necessarily, into words) according to rules in the merges file, and then converted to integers using the vocab file."},{"metadata":{},"cell_type":"markdown","source":"![](https://imgur.com/WkMWjRV.jpg)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import transformers\n\n# Tokenizer does the encoding to create the input ids\nfrom transformers import RobertaTokenizer\n\n# Use the 'Roberta Vocab File' dataset to get the vocab and merge files\ntokenizer = RobertaTokenizer(vocab_file = '/kaggle/input/roberta-vocab-file/vocab.json',\n                            merges_file = '/kaggle/input/roberta-vocab-file/merge.txt',\n                            lowercase = True,\n                            add_prefix_space = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2b. Creating input_ids and attention_mask"},{"metadata":{},"cell_type":"markdown","source":"Now we can use the tokenizer to encode the text from the dataset. \n\nThe maximum length of a BERT-based model input is 512. You can set MAX_LENGTH to be shorter, which can save space, by encoding all text strings in the training, validation, and test datasets, and seeing what the maximum length of an encoding is.\n\nThen we'll initialize arrays for the input_ids and attention_mask, based on the size of our data. "},{"metadata":{},"cell_type":"markdown","source":"![](https://imgur.com/FxGhdDm.jpg)"},{"metadata":{},"cell_type":"markdown","source":"# ** How to figure out what padding token to use"},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LENGTH = 512\n\n# input_ids is actually made of ones in this example because the padding token is 1 for Roberta, \n# but might be 0 for other BERT-based models\ntrain_input_ids = np.ones((X_train.shape[0], MAX_LENGTH), dtype = 'int32')\ntrain_attention_mask = np.zeros((X_train.shape[0], MAX_LENGTH), dtype = 'int32')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, loop throught the training data, use the tokenizer to encode it, and set the mask to be 1's at those same locations"},{"metadata":{},"cell_type":"markdown","source":"![](https://imgur.com/7dNHM54.jpg)"},{"metadata":{"trusted":true},"cell_type":"code","source":"for k in range(X_train.shape[0]):\n    encode = tokenizer.encode(X_train.iloc[k])\n    train_input_ids[k, :len(encode)] = encode\n    train_attention_mask[k, :len(encode)] = 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have inputs BERT-based models can actually use."},{"metadata":{},"cell_type":"markdown","source":"# Step 3: Construct a Pytorch Neural Network"},{"metadata":{},"cell_type":"markdown","source":"## 3a. Create torch data loaders"},{"metadata":{},"cell_type":"markdown","source":"In order to use these in a PyTorch neural network, we need to greate pytorch dataloaders.\nTo get there from numpy arrays, we do:\n\nNumpy array -> Torch tensor -> Torch dataset -> Torch dataloader"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# When we build the torch datasets, we need to pass in the batch size\nbatch_size = 8\n\n# Make tensors. Making the data type long is important, since there will be an error without it\ntrain_input_ids = torch.tensor(train_input_ids, dtype = torch.long)\ntrain_attention_mask = torch.tensor(train_attention_mask, dtype = torch.long)\ntrain_label = torch.tensor(y_train, dtype = torch.long)\n\n# Make a torch dataset\ntrain_t = torch.utils.data.TensorDataset(train_input_ids, train_attention_mask, train_label)\n\n# Make a torch dataloader.\ntrain_loader = torch.utils.data.DataLoader(train_t, batch_size = batch_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3b. Build a neural network"},{"metadata":{"trusted":true},"cell_type":"code","source":"# For the neural network\nimport torch.nn as nn\n\n# For the RobertaConfig\nfrom transformers import *\n\n# For some elements of the neural network\nimport torch.nn.functional as F","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ** What are some other configurations/ways to initialize?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# The Roberta Base dataset has a configuration file for roberta\nPATH = '/kaggle/input/roberta-base/'\nconfig = RobertaConfig.from_pretrained(PATH + 'config.json')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        \n        # The base RoBERTa model\n        self.roberta = RobertaModel.from_pretrained(PATH + 'pytorch_model.bin', config = config)\n        \n        # Update weights during training\n        for param in self.roberta.parameters():\n            param.requires_grad = True\n        \n        # A dropout layer\n        self.drop_out = nn.Dropout()\n        \n        # A fully connected layer. 768 is the size of the output, and 3 is the number of classes\n        self.fc = nn.Linear(768, 3)\n        \n    def forward(self, input_ids, input_mask):\n        \n        # Get the RoBERTa output\n        last_hidden_state, _ = self.roberta(input_ids, input_mask)\n        \n        # Get the CLS token, which holds the embedding for the text as a whole\n        last_hidden_state = last_hidden_state[:, 0, :] # indices are : (all batches), 0 (for the CLS token), : (all 768 elements of the output)\n        \n        # Dropout and fully connected layers\n        out = self.drop_out(last_hidden_state)\n        out = self.fc(out)\n        \n        return out","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 4: Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize the network\nmodel = Net()\n\n# Set the neural network to run on the GPU\nmodel.cuda()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4a. Pick hyperparameters"},{"metadata":{},"cell_type":"markdown","source":"In addition to the structure of the neural network, you choose the learning rate and loss function. It's typical to use cross-entropy loss for classification tasks, but different learning rates should be tested with cross validation."},{"metadata":{},"cell_type":"markdown","source":"# ** What is the difference between optimizers??"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pick a learning rate. This is a parameter you can tune yourself with cross-validation\nlearning_rate = 1e-5\n\n# Pick a loss function. Usually crossentropy loss for classification tasks\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# Pick an optimizer. This determines how the neural network converges to a solution\nopt = torch.optim.Adam(model.parameters(), lr = learning_rate)\n\n# Pick a number of epochs for which to train the model\nn_epochs = 2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4b. Train the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"for epoch in range(n_epochs):\n    \n    for i, batch in enumerate(train_loader):\n        \n        # A batch from the data loader. It has the input_ids, attention_mask, and labels\n        # Make sure to send each of these things to the GPU\n        batch = tuple(t.cuda() for t in batch)\n        \n        # Extract the input_ids, attention_mask, and labels from the batch\n        input_ids = batch[0]\n        attention_mask = batch[1]\n        labels = batch[2]\n        \n        # Use the model to make predictions\n        y_pred = model(input_ids, attention_mask)\n\n        # Calculate loss using the chosen loss function\n        loss = loss_fn(y_pred, labels)\n        \n        # Move in the direction of the gradient\n        opt.zero_grad()\n        loss.backward()\n        opt.step()\n        \n        # Status updates\n        print('Epoch {}/{} | Batch {}/{} | Loss: {:.4f}'.format(\n                epoch + 1, n_epochs, i, X_train.shape[0]/batch_size, loss))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 5: Validation"},{"metadata":{},"cell_type":"markdown","source":"All the steps done to convert the training data into the pytorch data loader need to be repeated for the validation set. Some people build the data loaders for the training, validation, and test sets at the same point in their notebook, but I've spread them out here so that we can go step-by-step.\n\nI won't comment each of these steps, since they're the same steps we followed above for the training set."},{"metadata":{},"cell_type":"markdown","source":"## 5a. Build PyTorch datasets for validation data"},{"metadata":{"trusted":true},"cell_type":"code","source":"ct = X_val.shape[0]\n\nval_input_ids = np.ones((ct, MAX_LENGTH), dtype = 'int32')\nval_attention_mask = np.zeros((ct, MAX_LENGTH), dtype = 'int32')\n\nfor k in range(X_val.shape[0]):\n    encode = tokenizer.encode(X_val.iloc[k])\n    val_input_ids[k, :len(encode)] = encode\n    val_attention_mask[k, :len(encode)] = 1\n\n\nval_input_ids = torch.tensor(val_input_ids, dtype = torch.long)\nval_attention_mask = torch.tensor(val_attention_mask, dtype = torch.long)\nval_label = torch.tensor(y_val, dtype = torch.long)\n\nval_t = torch.utils.data.TensorDataset(val_input_ids, val_attention_mask, val_label)\n\nval_loader = torch.utils.data.DataLoader(val_t, batch_size = batch_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5b. Make predictions for the validation set"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Holds the predictions\npreds = []\n\n# Set the model to evaluation mode\nmodel.eval()\n\n# torch.no_grad so the weights aren't updated\nwith torch.no_grad():\n\n    # Code below here is the same as during training\n    for i, batch in enumerate(val_loader):\n\n        # Get a batch from the validation data loader\n        batch = tuple(t.cuda() for t in batch)\n        input_ids = batch[0]\n        attention_mask = batch[1]\n        labels = batch[2]\n\n        # Make predictions, which will be probabilities of being in each class\n        y_pred = model(input_ids, attention_mask)\n\n        # Track the predictions \n        preds[i * batch_size:(i + 1) * batch_size] = F.softmax(y_pred, dim=1).detach().cpu().numpy()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5c. Evaluate performance"},{"metadata":{},"cell_type":"markdown","source":"You can evaluate the performance using any metric you'd like. Here, I use sklearn's classification report."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = np.argmax(preds, axis = 1)\n\nprint(classification_report(y_pred, y_val))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 6: Make predictions for the test set"},{"metadata":{},"cell_type":"markdown","source":"Once you think you've maximized the cross-validation score, use the model to make predictions for the test set, following the same steps that we did in step 5."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = test['text']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ct = X_test.shape[0]\n\ntest_input_ids = np.ones((ct, MAX_LENGTH), dtype = 'int32')\ntest_attention_mask = np.zeros((ct, MAX_LENGTH), dtype = 'int32')\n\nfor k in range(X_test.shape[0]):\n    encode = tokenizer.encode(X_test.iloc[k])\n    test_input_ids[k, :len(encode)] = encode\n    test_attention_mask[k, :len(encode)] = 1\n\n# Note that there's no label for the test set\ntest_input_ids = torch.tensor(test_input_ids, dtype = torch.long)\ntest_attention_mask = torch.tensor(test_attention_mask, dtype = torch.long)\n\ntest_t = torch.utils.data.TensorDataset(test_input_ids, test_attention_mask)\n\ntest_loader = torch.utils.data.DataLoader(test_t, batch_size = batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Holds the predictions\ntest_preds = []\n\n# Set the model to evaluation mode\nmodel.eval()\n\n# torch.no_grad so the weights aren't updated\nwith torch.no_grad():\n\n    # Code below here is the same as during training\n    for i, batch in enumerate(test_loader):\n\n        # Get a batch from the test data loader\n        batch = tuple(t.cuda() for t in batch)\n        input_ids = batch[0]\n        attention_mask = batch[1]\n        labels = batch[2]\n\n        # Make predictions, which will be probabilities of being in each class\n        y_pred = model(input_ids, attention_mask)\n\n        # Track the predictions \n        test_preds[i * batch_size:(i + 1) * batch_size] = F.softmax(y_pred, dim=1).detach().cpu().numpy()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 7: Submit predictions"},{"metadata":{},"cell_type":"markdown","source":"Note that classification ot tweets was actually not the goal of this competition. But to make submissions, code should look something like the following."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test = np.argmax(test_preds, axis = 1)\n\nsample['label'] = y_test\n\nsample.to_csv('submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Example 2: Question-Answering"},{"metadata":{},"cell_type":"markdown","source":"For QA questions, we encode the data in the same way. The way the questions are answered is by taking the dot product of each word encoding with a 'start' vector and an 'end' vector which will predict the probability that those words are the start or end of the answer.\n\nThe 'start vector' and 'end vector' are really just the weights for a 1D convolutional layer (since a 1d convolution with weights is equivalent to a dot-product with a vector)."},{"metadata":{},"cell_type":"markdown","source":"![](https://imgur.com/j7B3C0M.jpg)"},{"metadata":{},"cell_type":"markdown","source":"# ** How to do 5-fold cross validation (evaluate on out of fold sample, etc) "},{"metadata":{},"cell_type":"markdown","source":"# Step 1: Import data, split into training and validation set "},{"metadata":{},"cell_type":"markdown","source":"In the big picture, the way question-answering problems work is that you provide a context, like a Wikipedia article, and a question that can be answered using text from the context. \n\nIn this competition, the goal was to predict the part of each tweet that could best identify its sentiment. So the \"question\" is the sentiment, the \"answer\" was the selected portion of the tweet that identified the sentiment, and the \"context\" is the entire tweet."},{"metadata":{},"cell_type":"markdown","source":"In order to use BERT-based models for question answering, we need to structure our inputs a little bit differently, as shown in the following image."},{"metadata":{},"cell_type":"markdown","source":"![](https://imgur.com/asOb599.jpg)"},{"metadata":{},"cell_type":"markdown","source":"Our input id's are now the question and context separated by an [SEP] token, and we have two outputs, which are predictions for the tokens that start and end the answer."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\ntrain = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\ntest = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')\nsample = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/sample_submission.csv')\n\ntrain = train.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\n\nfeatures = train[['text', 'selected_text', 'sentiment']]\ny = train['selected_text']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split into a training set and a validation set\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(features, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 512\n\nct = X_train.shape[0]\ninput_ids = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\nstart_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\nend_tokens = np.zeros((ct,MAX_LEN),dtype='int32')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1a. Build the input_ids and attention_mask"},{"metadata":{},"cell_type":"markdown","source":"Note that we use 0 for the CLS token and 2 for the SEP token. These may change based on the model you're using. But you can check using the following example."},{"metadata":{"trusted":true},"cell_type":"code","source":"import transformers\n\n# Tokenizer does the encoding to create the input ids\nfrom transformers import RobertaTokenizer\n\n# Use the 'Roberta Vocab File' dataset to get the vocab and merge files\ntokenizer = RobertaTokenizer(vocab_file = '/kaggle/input/roberta-vocab-file/vocab.json',\n                            merges_file = '/kaggle/input/roberta-vocab-file/merge.txt',\n                            lowercase = True,\n                            add_prefix_space = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encoding for the cls token\ntokenizer.convert_tokens_to_ids(tokenizer.cls_token)\n\n# Encoding for the sep token\ntokenizer.convert_tokens_to_ids(tokenizer.sep_token)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# So we can loop through\nX_train.reset_index(inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for k in range(X_train.shape[0]):\n    \n    # encode the text\n    enc = tokenizer.encode(' '.join(X_train.loc[k, 'text'].split()))\n    \n    # get the token for the current sentiment\n    s_tok = tokenizer.encode(X_train.loc[k,'sentiment'])\n    s_tok = s_tok[1] # Ignore the cls and sep tokens\n    \n    input_ids[k,:len(enc)+2] = enc + [s_tok] + [2]\n    attention_mask[k,:len(enc)+2] = 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1b. Build arrays to track the tokens associated with the start and end of the selected text"},{"metadata":{},"cell_type":"markdown","source":"This logic is borrowed from this notebook: Logic borrowed from here: https://www.kaggle.com/nkoprowicz/tensorflow-roberta-0-705/edit. I've just done my best to dissect it. \n\nThis isn't as straightforward as it might seem, since again, there's not a 1-1 correspondence between the encoded values and the words in the tweets. So here's how the logic works:"},{"metadata":{},"cell_type":"markdown","source":"![](https://imgur.com/AV8MuMq.jpg)"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = y_train.reset_index(drop = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.iloc[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\nend_tokens = np.zeros((ct,MAX_LEN),dtype='int32')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for k in range(X_train.shape[0]):\n    # FIND OVERLAP\n    text1 = \" \" + \" \".join(X_train.loc[k,'text'].split()) # You need the extra space at the beginning because when the first token is decoded later, it will add a space before the first word\n    text2 = \" \".join(y_train[k].split())\n    \n    #print(text1)\n    #print(text2)\n    idx = text1.find(text2) # get the index of the first character where there's overlap\n    \n    # Initialize chars array\n    chars = np.zeros((len(text1)))\n    \n    chars[idx:idx+len(text2)]=1 # set to 1 everywhere there's overlap\n    print(chars)\n    if text1[idx-1]==' ': chars[idx-1] = 1 \n     \n    #print(chars)\n    # Encode the context\n    enc = tokenizer.encode(text1) \n    print(enc)\n    # Build the offsets array\n    offsets = []\n    idx=0\n    for t in enc[1:-1]:\n        w = tokenizer.decode([t]) # get the characters for the current token\n        print(w)\n        offsets.append((idx,idx+len(w))) # append the incides where those characters start and end\n        idx += len(w) # move the index to the next index, which will be the first character of the next decoded chunk\n        print(offsets)\n        print(len(w))\n        print(' ')\n    \n    # Make the start and end tokens\n    # toks will track all the tokens that overlap with the selected text\n    print(offsets)\n    \n    toks = []\n    for i,(a,b) in enumerate(offsets):\n        sm = np.sum(chars[a:b])\n        if sm>0: toks.append(i) \n\n    # The start token is the first element of toks and the end index is the last element\n    # We add 1 to account for the CLS token\n    #print(toks)\n    print(toks)\n    if len(toks)>0:\n     #   print(toks[0])\n     #   print(toks[-1])\n     #   print(' ')\n        start_tokens[k,toks[0] + 1] = 1\n        end_tokens[k,toks[-1] + 1] = 1\n        \n    break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.loc[0, ['selected_text', 'text']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer.encode(' '.join(X_train.loc[0, 'text'].split()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer.decode([1437])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\nend_tokens = np.zeros((ct,MAX_LEN),dtype='int32')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"end_tokens[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# When we build the torch datasets, we need to pass in the batch size\nbatch_size = 8\n\n# Make tensors. Making the data type long is important, since there will be an error without it\ntrain_input_ids = torch.tensor(input_ids, dtype = torch.long)\ntrain_attention_mask = torch.tensor(attention_mask, dtype = torch.long)\nstart_labels = torch.tensor(start_tokens, dtype = torch.long)\nend_labels = torch.tensor(end_tokens, dtype = torch.long)\n\n# Make a torch dataset\ntrain_t = torch.utils.data.TensorDataset(train_input_ids, train_attention_mask, start_labels, end_labels)\n\n# Make a torch dataloader.\ntrain_loader = torch.utils.data.DataLoader(train_t, batch_size = batch_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 2: Build the neural network"},{"metadata":{},"cell_type":"markdown","source":"For a loss function, we'll just try to predict the start and end and add the losses"},{"metadata":{"trusted":true},"cell_type":"code","source":"# For the neural network\nimport torch.nn as nn\n\n# For the RobertaConfig\nfrom transformers import *\n\n# For some elements of the neural network\nimport torch.nn.functional as F","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The Roberta Base dataset has a configuration file for roberta\nPATH = '/kaggle/input/roberta-base/'\nconfig = RobertaConfig.from_pretrained(PATH + 'config.json')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        \n        # The base RoBERTa model\n        self.roberta = RobertaModel.from_pretrained(PATH + 'pytorch_model.bin', config = config)\n        \n        # Update weights during training\n        for param in self.roberta.parameters():\n            param.requires_grad = True\n        \n        # A dropout layer\n        self.drop_out = nn.Dropout()\n        \n        # A fully connected layer. 768 is the size of the output, and 3 is the number of classes\n        self.fc = nn.Linear(768, 2) # size 2 for the start and end\n        \n    def forward(self, input_ids, input_mask):\n        \n        # Get the RoBERTa output\n        last_hidden_state, _ = self.roberta(input_ids, input_mask)\n        \n        # Dropout and fully connected layers\n        out = self.drop_out(last_hidden_state) # NOTE: We use the whole hidden layer, not just the embedding for the CLS token\n        out = self.fc(out)\n        \n        start_logits, end_logits = out.split(1, dim=-1) # split the output to get the start and end logits\n        \n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n        \n        return start_logits, end_logits","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize the network\nmodel = Net()\n\n# Set the neural network to run on the GPU\nmodel.cuda()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pick a learning rate. This is a parameter you can tune yourself with cross-validation\nlearning_rate = 1e-5\n\n# Pick an optimizer. This determines how the neural network converges to a solution\nopt = torch.optim.Adam(model.parameters(), lr = learning_rate)\n\n# Pick a number of epochs for which to train the model\nn_epochs = 1\n\n# For the loss function, add the cross entropy loss for the start and end tokens\ndef loss_fn(start_preds, end_preds, start_tokens, end_tokens):\n    ce_loss = nn.CrossEntropyLoss()\n    \n    start_loss = ce_loss(start_logits, start_tokens)\n    end_loss = ce_loss(end_logits, end_tokens)    \n    total_loss = start_loss + end_loss\n    return total_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start_preds = []\nend_preds = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.train()\n\nfor epoch in range(n_epochs):\n    \n    for i, batch in enumerate(train_loader):\n        \n        # A batch from the data loader. It has the input_ids, attention_mask, and labels\n        # Make sure to send each of these things to the GPU\n        batch = tuple(t.cuda() for t in batch)\n        \n        # Extract the input_ids, attention_mask, and labels from the batch\n        input_ids = batch[0]\n        attention_mask = batch[1]\n        \n        # Now we have the start and end tokens\n        start_tokens = torch.argmax(batch[2], dim = 1) # Cross entropy loss wants the index of the answer, not the whole array\n        end_tokens = torch.argmax(batch[3], dim = 1)\n        \n        # Use the model to make predictions\n        start_logits, end_logits = model(input_ids, attention_mask)\n        \n        # Calculate loss using the chosen loss function\n        loss = loss_fn(start_logits, end_logits, start_tokens, end_tokens)\n        \n        # Move in the direction of the gradient\n        opt.zero_grad()\n        loss.backward()\n        opt.step()\n        \n        # Status updates\n        print('Epoch {}/{} | Batch {}/{} | Loss: {:.4f}'.format(\n                epoch + 1, n_epochs, i, X_train.shape[0]/batch_size, loss))\n        \n        start_preds[batch_size*i:batch_size*i + batch_size-1] = start_logits.argmax(dim = 1).cpu().detach().numpy()\n        end_preds[batch_size*i:batch_size*i + batch_size-1] = end_logits.argmax(dim = 1).cpu().detach().numpy()\n    \n        '''\n        print(tokenizer.decode(input_ids[0]))\n        print(start_logits.size())\n        print(start_logits[0].argmax())\n        print(tokenizer.decode([input_ids[0][start_logits[0].argmax()]]))\n        print(' ')\n        print(end_logits[0].argmax())\n        print(tokenizer.decode([input_ids[0][end_logits[0].argmax()]]))\n        '''\n        if i == 100:\n            break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"end_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer.encode(' ' + ' '.join()' not  you, me, just drank too much.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train['predicted_text'] = ' '\n\njaccard_scores = []\n\nfor k in range(100):\n    \n    # encode the text\n    enc = tokenizer.encode(' ' + ' '.join(X_train.loc[k, 'text'].split()))\n    print(X_train.loc[k, 'text'])\n    print(enc)\n    X_train.loc[k, 'predicted_text'] = tokenizer.decode(enc[start_preds[k]:end_preds[k] + 1]) # Need the + 1 since we predicted what should be the last word, but slicing cuts off 1 before that\n    print(tokenizer.decode(enc[start_preds[k]:end_preds[k]]))\n        \n    jaccard_scores.append(jaccard(X_val.loc[k, 'selected_text'], X_val.loc[k, 'predicted_text']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# How to save model weights to not have to retrain model each time??"},{"metadata":{},"cell_type":"markdown","source":"Always download the weights so you don't have to train the model again"},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.save(model.state_dict(), 'model_weights')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Validation Set"},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 512\n\nct = X_val.shape[0]\nval_input_ids = np.ones((ct,MAX_LEN),dtype='int32')\nval_attention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\nval_start_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\nval_end_tokens = np.zeros((ct,MAX_LEN),dtype='int32')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# So we can loop through\nX_val.reset_index(inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for k in range(X_val.shape[0]):\n    \n    # encode the text\n    enc = tokenizer.encode(' '.join(X_val.loc[k, 'text'].split()))\n    \n    # get the token for the current sentiment\n    s_tok = tokenizer.encode(X_val.loc[k,'sentiment'])\n    s_tok = s_tok[1] # Ignore the cls and sep tokens\n    \n    val_input_ids[k,:len(enc)+2] = enc + [s_tok] + [2]\n    val_attention_mask[k,:len(enc)+2] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# When we build the torch datasets, we need to pass in the batch size\nbatch_size = 8\n\n# Make tensors. Making the data type long is important, since there will be an error without it\nval_input_ids = torch.tensor(val_input_ids, dtype = torch.long)\nval_attention_mask = torch.tensor(val_attention_mask, dtype = torch.long)\n\n# Make a torch dataset\nval_t = torch.utils.data.TensorDataset(val_input_ids, val_attention_mask)\n\n# Make a torch dataloader.\nval_loader = torch.utils.data.DataLoader(val_t, batch_size = batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start_preds = []\nend_preds = []\n\n# Set the model to evaluation mode\nmodel.eval()\n\n# torch.no_grad so the weights aren't updated\nwith torch.no_grad():\n    \n    for i, batch in enumerate(val_loader):\n\n        batch = tuple(t.cuda() for t in batch)\n\n        # Extract the input_ids, attention_mask, and labels from the batch\n        input_ids = batch[0]\n        attention_mask = batch[1]\n\n        # Use the model to make predictions\n        start_logits, end_logits = model(input_ids, attention_mask)\n\n        \n        #print(input_ids)\n        #print(attention_mask)\n        \n        #a = torch.softmax(start_logits[0], dim = 0)\n        #b = torch.softmax(end_logits[0], dim = 0)\n        #print(a)\n        #print(b)\n        \n        #start_idx = start_logits.argmax(dim = 1)\n        #end_idx = end_logits.argmax(dim = 1)\n        \n        #start_idx = a.argmax(dim = 0)\n        #end_idx = b.argmax(dim = 0)\n\n        start_preds[batch_size*i:batch_size*i + batch_size-1] = start_logits.argmax(dim = 1).cpu().detach().numpy()\n        end_preds[batch_size*i:batch_size*i + batch_size-1] = end_logits.argmax(dim = 1).cpu().detach().numpy()\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start_preds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Metric"},{"metadata":{"trusted":true},"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    if (len(a)==0) & (len(b)==0): return 0.5\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_val['predicted_text'] = ' '\n\njaccard_scores = []\n\nfor k in range(X_val.shape[0]):\n    \n    # encode the text\n    enc = tokenizer.encode(' ' + ' '.join(X_val.loc[k, 'text'].split()))\n    \n    X_val.loc[k, 'predicted_text'] = tokenizer.decode(enc[start_preds[k]:end_preds[k] + 1])\n        \n    jaccard_scores.append(jaccard(X_val.loc[k, 'selected_text'], X_val.loc[k, 'predicted_text']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_val.sample(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start_preds[3489]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"end_preds[3489]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_val.loc[3489, 'text']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"b = tokenizer.encode(X_val.loc[3489, 'text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\" \".join(tokenizer.decode(b[5:7]).split())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.mean(jaccard_scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Upgrades"},{"metadata":{},"cell_type":"markdown","source":"By now, you understand how BERT-based methods work and can be used for classification or question-answering tasks. In order to show detail in the examples above, I left things simple, and didn't try to condense any code. Now we'll learn some strategies for making things easier and improving."},{"metadata":{},"cell_type":"markdown","source":"## 1. Doing 5-fold CV while training instead of splitting into training set and validation set from the get-go"},{"metadata":{"trusted":true},"cell_type":"code","source":"for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df.sentiment), start=1): \n    print(f'Fold: {fold}')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}